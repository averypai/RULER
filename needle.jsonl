{"idx": "19130332", "QUESTION": "Is the zeolite hemostatic agent beneficial in reducing blood loss during arterial injury?", "CONCLUSION": "Uncontrolled hemorrhage is the leading cause of fatality. The aim of this study was to evaluate the effect of zeolite mineral (QuikClot - Advanced Clotting Sponge [QC-ACS]) on blood loss and physiological variables in a swine extremity arterial injury model.\nSixteen swine were used. Oblique groin incision was created and a 5 mm incision was made. The animals were allocated to: control group (n: 6): Pressure dressing was applied with manual pressure over gauze sponge; or QC group (n: 10): QC was directly applied over lacerated femoral artery. Mean arterial pressure, blood loss and physiological parameters were measured during the study period.\nApplication of QC led to a slower drop in blood pressure. The control group had a significantly higher increase in lactate within 60 minutes. The mean prothrombin time in the control group was significantly increased at 60 minutes. The application of QC led to decreased total blood loss. The QC group had significantly higher hematocrit levels. QC application generated a significant heat production. There were mild edematous and vacuolar changes in nerve samples.\nAccording to the physiological parameters, we observed that zeolite tends to reduce blood loss, however could not stop bleeding completely. We believe that further clinical trials are needed to conclude that zeolite could be used in the routine practice.\n", "TYPE": ["Administration, Topical", "Animals", "Bandages", "Blood Pressure", "Disease Models, Animal", "Femoral Artery", "Hematocrit", "Hemorrhage", "Hemostatics", "Random Allocation", "Specific Pathogen-Free Organisms", "Survival Rate", "Swine", "Zeolites"], "FINAL_DECISION": "yes"}
{"idx": "9427037", "QUESTION": "Are endothelial cell patterns of astrocytomas indicative of grade?", "CONCLUSION": "The most common primary brain tumors in children and adults are of astrocytic origin. Classic histologic grading schemes for astrocytomas have included evaluating the presence or absence of nuclear abnormalities, mitoses, vascular endothelial proliferation, and tumor necrosis.\nWe evaluated the vascular pattern of 17 astrocytoma surgical specimens (seven from children and 10 from adults), and four normal brains obtained at autopsy, utilizing antibody to glial fibrillary acidic protein (GFAP) and von Willebrand factor (vWF) utilizing confocal microscopy. A modified WHO classification was used.\nAll tumor cases showed cells positive for GFAP. Control tissues showed a few, widely separated vessels. Pilocytic astrocytomas (four cases) showed lacy clusters of small-to-medium sized vessels, with intact vessel wall integrity. Diffuse, low grade astrocytoma (three cases) showed a staining pattern similar to control tissue; intermediate grade (one case), anaplastic astrocytoma (three cases) and gliobastoma multiforme (six cases) showed an increased vessel density with multiple small vessels (glomeruloid clusters), some with prominent intimal hyperplasia, loss of vessel wall integrity, and with numerous vWF-positive single cells/microvessels within the tumor substance.\nEvaluation of astrocytomas utilizing antibody to vWF and confocal microscopy aids in the grading of these neoplasms.\n", "TYPE": ["Adolescent", "Adult", "Astrocytoma", "Child, Preschool", "Endothelium, Vascular", "Female", "Fluorescent Antibody Technique, Indirect", "Glial Fibrillary Acidic Protein", "Humans", "Infant", "Male", "von Willebrand Factor"], "FINAL_DECISION": "yes"}
{"idx": "22680064", "QUESTION": "Can third trimester ultrasound predict the presentation of the first twin at delivery?", "CONCLUSION": "To determine the ability of early sonogram to predict the presentation of twin A at birth.\nA retrospective cohort study was conducted on all twin pregnancies evaluated at our Fetal Evaluation Unit from 2007 to 2009. Sonogram records were reviewed for the presentation of twin A at seven gestational age intervals and inpatient medical records were reviewed for the presentation of twin A at delivery. The positive predictive value, sensitivity, and specificity of presentation as determined by ultrasound, at each gestational age interval, for the same presentation at delivery were calculated.\nTwo hundred and thirty-eight twin pregnancies met inclusion criteria. A total of 896 ultrasounds were reviewed. The positive predictive value of cephalic presentation of twin A as determined by ultrasound for the persistence of cephalic presentation at delivery reached 95% after 28 weeks gestation. The positive predictive value for noncephalic presentation as established by sonogram for noncephalic at delivery was>90% after 32 weeks gestation.\nThe presentation of the first twin at delivery can be determined by sonogram by the 32nd week of gestation in over 90% of twin pregnancies.\n", "TYPE": ["Adult", "Birth Order", "Birth Weight", "Cohort Studies", "Delivery, Obstetric", "Female", "Humans", "Infant, Newborn", "Labor Presentation", "Predictive Value of Tests", "Pregnancy", "Pregnancy Trimester, Third", "Pregnancy, Twin", "Retrospective Studies", "Twins", "Ultrasonography, Prenatal"], "FINAL_DECISION": "yes"}
{"idx": "21726930", "QUESTION": "Is endometrial polyp formation associated with increased expression of vascular endothelial growth factor and transforming growth factor-beta1?", "CONCLUSION": "Endometrial polyp is a common cause of abnormal uterine bleeding, but the etiology and pathogenesis remain unclear. Vascular endothelial growth factor (VEGF) is angiogenic, related to thick walled vessels and transforming growth factor-beta1 (TGF-\u03b21) is related to fibrotic tissue, which are characteristics of endometrial polyps. The primary objective of this study was to find out if endometrial polyp formation is associated with increased expression of VEGF or TGF-\u03b21, or both. A secondary objective is to determine if the changes are related to steroid receptor expression.\nThis prospective study compared VEGF and TGF-\u03b21 expression of endometrial polyps and adjacent endometrial tissue in 70 premenopausal women. The comparison of results was separately made for endometrium specimens obtained in the proliferative and secretory phases. The results were correlated with the steroid receptors (estrogen receptor and progesterone receptor) expression.\nThe score of VEGF in glandular cells of endometrial polyps was significantly higher than the score in adjacent endometrium, both in the proliferative phase (P<0.001) and the secretory phase (P=0.03); the score of VEGF in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium only in proliferative phase (P=0.006). The score of TGF-\u03b21 in glandular cells of endometrial polyps was significantly higher than the score in adjacent endometrium in proliferative phase (P=0.02); whereas the score of TGF-\u03b21 in stromal cells of endometrial polyps was significantly higher than the score in adjacent endometrium, both in the proliferative phase (P=0.006) and the secretory phase (P=0.008). There was a significant correlation between the expression of steroid receptors and VEGF and TGF-\u03b21 (Spearman's correlation P<0.001 and P<0.05, respectively).\nThere was increased expression of TGF-\u03b21 and VEGF in polyps compared to adjacent normal endometrial tissue. It suggested that these cytokines might play a role in endometrial polyp formation. In addition, there was a significant correlation between steroid receptor expression and VEGF and TGF-\u03b21 expression.\n", "TYPE": ["Adult", "Biopsy", "Endometrium", "Female", "Follicular Phase", "Humans", "Hysteroscopy", "Immunohistochemistry", "Luteal Phase", "Middle Aged", "Polyps", "Prospective Studies", "Receptors, Estrogen", "Receptors, Progesterone", "Stromal Cells", "Transforming Growth Factor beta1", "Up-Regulation", "Uterine Diseases", "Vascular Endothelial Growth Factor A", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "26370095", "QUESTION": "Are financial incentives cost-effective to support smoking cessation during pregnancy?", "CONCLUSION": "To investigate the cost-effectiveness of up to \u00a3400 worth of financial incentives for smoking cessation in pregnancy as an adjunct to routine health care.\nCost-effectiveness analysis based on a Phase II randomized controlled trial (RCT) and a cost-utility analysis using a life-time Markov model.\nThe RCT was undertaken in Glasgow, Scotland. The economic analysis was undertaken from the UK National Health Service (NHS) perspective.\nA total of 612 pregnant women randomized to receive usual cessation support plus or minus financial incentives of up to \u00a3400 vouchers (US $609), contingent upon smoking cessation.\nComparison of usual support and incentive interventions in terms of cotinine-validated quitters, quality-adjusted life years (QALYs) and direct costs to the NHS.\nThe incremental cost per quitter at 34-38 weeks pregnant was \u00a31127 ($1716).This is similar to the standard look-up value derived from Stapleton&West's published ICER tables, \u00a31390 per quitter, by looking up the Cessation in Pregnancy Incentives Trial (CIPT) incremental cost (\u00a3157) and incremental 6-month quit outcome (0.14). The life-time model resulted in an incremental cost of \u00a317 [95% confidence interval (CI)\u2009=\u2009-\u00a393, \u00a3107] and a gain of 0.04 QALYs (95% CI\u2009=\u2009-0.058, 0.145), giving an ICER of \u00a3482/QALY ($734/QALY). Probabilistic sensitivity analysis indicates uncertainty in these results, particularly regarding relapse after birth. The expected value of perfect information was \u00a330 million (at a willingness to pay of \u00a330\u2009000/QALY), so given current uncertainty, additional research is potentially worthwhile.\nFinancial incentives for smoking cessation in pregnancy are highly cost-effective, with an incremental cost per quality-adjusted life years of \u00a3482, which is well below recommended decision thresholds.\n", "TYPE": ["Cost-Benefit Analysis", "Female", "Health Promotion", "Humans", "Markov Chains", "Motivation", "Pregnancy", "Pregnancy Complications", "Prenatal Care", "Quality-Adjusted Life Years", "Scotland", "Smoking", "Smoking Cessation", "Smoking Prevention"], "FINAL_DECISION": "yes"}
{"idx": "11146778", "QUESTION": "Risk stratification in emergency surgical patients: is the APACHE II score a reliable marker of physiological impairment?", "CONCLUSION": "The APACHE II (Acute Physiology and Chronic Health Evaluation II) score used as an intensive care unit (ICU) admission score in emergency surgical patients is not independent of the effects of treatment and might lead to considerable bias in the comparability of defined groups of patients and in the evaluation of treatment policies. Postoperative monitoring with the APACHE II score is clinically irrelevant.\nInception cohort study.\nSecondary referral center.\nEighty-five consecutive emergency surgical patients admitted to the surgical ICU in 1999. The APACHE II score was calculated before surgery; after admission to the ICU; and on postoperative days 3, 7, and 10.\nAPACHE II scores and predicted and observed mortality rates.\nThe mean +/- SD APACHE II score of 24.2 +/- 8.3 at admission to the ICU was approximately 36% greater than the initial APACHE II score of 17.8 +/- 7.7, a difference that was highly statistically significant (P<.001). The overall mortality of 32% favorably corresponds with the predicted mortality of 34% according to the initial APACHE II score. However, the predicted mortality of 50% according to the APACHE II score at admission to the ICU was significantly different from the observed mortality rate (P =.02). In 40 long-term patients (>/=10 days in the ICU), the difference between the APACHE II scores of survivors and patients who died was statistically significant on day 10 (P =.04).\nFor risk stratification in emergency surgical patients, it is essential to measure the APACHE II score before surgical treatment. Longitudinal APACHE II scoring reveals continuous improvement of the score in surviving patients but has no therapeutic relevance in the individual patient.\n", "TYPE": ["APACHE", "Aged", "Cohort Studies", "Emergency Treatment", "Female", "Humans", "Intensive Care Units", "Male", "Preoperative Care", "Risk Assessment", "Surgical Procedures, Operative", "Survival Rate", "Time Factors"], "FINAL_DECISION": "yes"}
{"idx": "21850494", "QUESTION": "Hepatorenal syndrome: are we missing some prognostic factors?", "CONCLUSION": "Hepatorenal syndrome (HRS) is the functional renal failure associated with advanced cirrhosis and has also been described in fulminant hepatic failure. Without liver transplantation its prognosis is dismal. Our study included patients with type 1 HRS associated with cirrhosis, who were not liver transplant candidates.AIM: To identify variables associated with improved survival.\nSixty-eight patients fulfilled the revised Ascites Club Criteria for type 1 HRS. None of them was suitable for liver transplantation. All the patients were treated with combinations of: albumin, midodrine and octreotide, pressors, and hemodialysis.\nMedian survival was 13 days for the whole group. Survival varied with the end-stage liver disease (ESLD) etiology: autoimmune, 49 days, cardiac cirrhosis, 22 days, idiopathic, 15.5 days, viral, 15 days, hepatitis C and alcohol, 14.5 days, alcohol 8 days, and neoplasia 4 days (p = 0.048). Survival of HRS associated with alcoholic liver disease versus other etiologies was not statistically significant (p = 0.1). Increased serum creatinine (p = 0.02) and urinary sodium 6-10 mEq/l (p = 0.027) at the initiation of therapy were prognostic factors for mortality. HRS treatment modalities (p = 0.73), use of dialysis (p = 0.56), dialysis modality (p = 0.35), use of vasopressors (p = 0.26), pre-existing renal disease (p = 0.49), gender (p = 0.90), and age (p = 0.57) were not associated with survival.\nWe report for the first time ESLD etiology as a prognostic factor for survival. The renal function (expressed as serum creatinine) and urinary Na (<5 mEq/l) at the time of diagnosis were found to be associated with survival, suggesting that early treatment might increase survival.\n", "TYPE": ["Albumins", "Alcoholism", "Autoimmune Diseases", "Creatinine", "End Stage Liver Disease", "Female", "Hepatitis C", "Hepatorenal Syndrome", "Humans", "Liver Cirrhosis", "Male", "Middle Aged", "Midodrine", "Octreotide", "Prognosis", "Renal Dialysis", "Retrospective Studies", "Sodium", "Survival Rate"], "FINAL_DECISION": "yes"}
{"idx": "20577124", "QUESTION": "Is leptin involved in phagocytic NADPH oxidase overactivity in obesity?", "CONCLUSION": "Hyperleptinemia and oxidative stress play a major role in the development of cardiovascular diseases in obesity. This study aimed to investigate whether there is a relationship between plasma levels of leptin and phagocytic nicotinamide adenine dinucleotide phosphate (NADPH) oxidase activity, and its potential relevance in the vascular remodeling in obese patients.\nThe study was performed in 164 obese and 94 normal-weight individuals (controls). NADPH oxidase activity was evaluated by luminescence in phagocytic cells. Levels of leptin were quantified by ELISA in plasma samples. Carotid intima-media thickness (cIMT) was measured by ultrasonography. In addition, we performed in-vitro experiments in human peripheral blood mononuclear cells and murine macrophages.\nPhagocytic NADPH oxidase activity and leptin levels were enhanced (P<0.05) in obese patients compared with controls. NADPH oxidase activity positively correlated with leptin in obese patients. This association remained significant in a multivariate analysis. cIMT was higher (P<0.05) in obese patients compared with controls. In addition, cIMT also correlated positively with leptin and NADPH oxidase activity in obese patients. In-vitro studies showed that leptin induced NADPH oxidase activation. Inhibition of the leptin-induced NADPH oxidase activity by wortmannin and bisindolyl maleimide suggested a direct involvement of the phosphatidylinositol 3-kinase and protein kinase C pathways, respectively. Finally, leptin-induced NADPH oxidase activation promoted macrophage proliferation.\nThese findings show that phagocytic NADPH oxidase activity is increased in obesity and is related to preclinical atherosclerosis in this condition. We also suggest that hyperleptinemia may contribute to phagocytic NADPH oxidase overactivity in obesity.\n", "TYPE": ["Animals", "Atherosclerosis", "Carotid Arteries", "Case-Control Studies", "Cell Line", "Cell Proliferation", "Female", "Humans", "In Vitro Techniques", "Leptin", "Macrophages", "Male", "Mice", "Middle Aged", "NADPH Oxidases", "Obesity", "Oxidative Stress", "Phagocytes", "Superoxides", "Tunica Intima"], "FINAL_DECISION": "yes"}
{"idx": "16266387", "QUESTION": "Fast foods - are they a risk factor for asthma?", "CONCLUSION": "Lifestyle changes over the last 30 years are the most likely explanation for the increase in allergic disease over this period.AIM: This study tests the hypothesis that the consumption of fast food is related to the prevalence of asthma and allergy.\nAs part of the International Study of Asthma and Allergies in Childhood (ISAAC) a cross-sectional prevalence study of 1321 children (mean age = 11.4 years, range: 10.1-12.5) was conducted in Hastings, New Zealand. Using standard questions we collected data on the prevalence of asthma and asthma symptoms, as well as food frequency data. Skin prick tests were performed to common environmental allergens and exercise-induced bronchial hyperresponsiveness (BHR) was assessed according to a standard protocol. Body mass index (BMI) was calculated as weight/height2 (kg/m2) and classified into overweight and obese according to a standard international definition.\nAfter adjusting for lifestyle factors, including other diet and BMI variables, compared with children who never ate hamburgers, we found an independent risk of hamburger consumption on having a history of wheeze [consumption less than once a week (OR = 1.44, 95% CI: 1.06-1.96) and 1+ times a week (OR = 1.65, 95% CI: 1.07-2.52)] and on current wheeze [consumption less than once a week (OR = 1.17, 95% CI: 0.80-1.70) and 1+ times a week (OR = 1.81, 95% CI: 1.10-2.98)]. Takeaway consumption 1+ times a week was marginally significantly related to BHR (OR = 2.41, 95% CI: 0.99-5.91). There was no effect on atopy.\nFrequent consumption of hamburgers showed a dose-dependent association with asthma symptoms, and frequent takeaway consumption showed a similar association with BHR.\n", "TYPE": ["Adult", "Animals", "Asthma", "Beverages", "Bronchial Hyperreactivity", "Cattle", "Child", "Cross-Sectional Studies", "Diet", "Female", "Humans", "Male", "Meat Products", "Prevalence", "Respiratory Sounds", "Risk Factors", "Skin Tests"], "FINAL_DECISION": "yes"}
{"idx": "18594195", "QUESTION": "Do older patients who refuse to participate in a self-management intervention in the Netherlands differ from older patients who agree to participate?", "CONCLUSION": "Refusal of patients to participate in intervention programs is an important problem in clinical trials but, in general, researchers devote relatively little attention to it. In this article, a comparison is made between patients who, after having been invited, agreed to participate in a self-management intervention (participants) and those who refused (refusers). Compared with other studies of refusers, relatively more information could be gathered with regard to both their characteristics and reasons for refusing, because all potential participants were invited personally.\nOlder patients from a Dutch outpatient clinic were invited to participate in a self-management intervention, and their characteristics were assessed. Demographic data were collected, as well as data on physical functioning and lack of emotional support. People who refused to participate were asked to give their reasons for refusing.\nOf the 361 patients invited, 267 (74%) refused participation. These refusers were more restricted in their mobility, lived further away from the location of the intervention, and had a partner more often than did the participants. No differences were found in level of education, age or gender. The main reasons given by respondents for refusing to participate were lack of time, travel distance, and transport problems.\nAs in many studies, the refusal rate in this study is high, and seems to be related to physical mobility restrictions, travel distance and, partly, to availability of emotional support. These findings may be used to make the recruitment process more effective - for example, by offering transport to the location of the intervention.\n", "TYPE": ["Aged", "Female", "Humans", "Logistic Models", "Male", "Motor Activity", "Netherlands", "Patients", "Refusal to Participate", "Self Care", "Surveys and Questionnaires"], "FINAL_DECISION": "yes"}
{"idx": "23810330", "QUESTION": "Is intraoperative neuromonitoring associated with better functional outcome in patients undergoing open TME?", "CONCLUSION": "Intraoperative neuromonitoring (IONM) aims to control nerve-sparing total mesorectal excision (TME) for rectal cancer in order to improve patients' functional outcome. This study was designed to compare the urogenital and anorectal functional outcome of TME with and without IONM of innervation to the bladder and the internal anal sphincter.\nA consecutive series of 150 patients with primary rectal cancer were analysed. Fifteen match pairs with open TME and combined urogenital and anorectal functional assessment at follow up were established identical regarding gender, tumour site, tumour stage, neoadjuvant radiotherapy and type of surgery. Urogenital and anorectal function was evaluated prospectively on the basis of self-administered standardized questionnaires, measurement of residual urine volume and longterm-catheterization rate.\nNewly developed urinary dysfunction after surgery was reported by 1 of 15 patients in the IONM group and by 6 of 15 in the control group (p\u00a0=\u00a00.031). Postoperative residual urine volume was significantly higher in the control group. At follow up impaired anorectal function was present in 1 of 15 patients undergoing TME with IONM and in 6 of 15 without IONM (p\u00a0=\u00a00.031). The IONM group showed a trend towards a lower rate of sexual dysfunction after surgery.\nIn this study TME with IONM was associated with significant lower rates of urinary and anorectal dysfunction. Prospective randomized trials are mandatory to evaluate the definite role of IONM in rectal cancer surgery.\n", "TYPE": ["Aged", "Aged, 80 and over", "Anal Canal", "Autonomic Pathways", "Case-Control Studies", "Cohort Studies", "Fecal Incontinence", "Female", "Humans", "Lower Urinary Tract Symptoms", "Male", "Middle Aged", "Monitoring, Intraoperative", "Organ Sparing Treatments", "Peripheral Nerve Injuries", "Prospective Studies", "Rectal Neoplasms", "Rectum", "Sexual Dysfunction, Physiological", "Treatment Outcome", "Urinary Bladder"], "FINAL_DECISION": "yes"}
{"idx": "23736032", "QUESTION": "Multidisciplinary decisions in breast cancer: does the patient receive what the team has recommended?", "CONCLUSION": "A multidisciplinary team (MDT) approach to breast cancer management is the gold standard. The aim is to evaluate MDT decision making in a modern breast unit.\nAll referrals to the breast MDT where breast cancer was diagnosed from 1 July 2009 to 30 June 2011 were included. Multidisciplinary team decisions were compared with subsequent patient management and classified as concordant or discordant.\nOver the study period, there were 3230 MDT decisions relating to 705 patients. Overall, 91.5% (2956 out of 3230) of decisions were concordant, 4.5% (146 out of 3230), were discordant and 4% (128 out of 3230) had no MDT decision. Of 146 discordant decisions, 26 (17.8%) were considered 'unjustifiable' as there was no additional information available after the MDT to account for the change in management. The remaining 120 discordant MDT decisions were considered 'justifiable', as management was altered due to patient choice (n=61), additional information available after MDT (n=54) or MDT error (n=5).\nThe vast majority of MDT decisions are implemented. Management alteration was most often due to patient choice or additional information available after the MDT. A minority of management alterations were 'unjustifiable' and the authors recommend that any patient whose treatment is subsequently changed should have MDT rediscussion prior to treatment.\n", "TYPE": ["Breast Neoplasms", "Carcinoma", "Choice Behavior", "Decision Making", "Female", "Guideline Adherence", "Humans", "Interdisciplinary Communication", "Medical Errors", "Patient Access to Records", "Patient Care Team", "Patient Compliance", "Patient Education as Topic", "Physician-Patient Relations", "Referral and Consultation", "Retrospective Studies"], "FINAL_DECISION": "yes"}
{"idx": "28143468", "QUESTION": "Are performance measurement systems useful?", "CONCLUSION": "Prior literature identified the use of Performance Measurement Systems (PMS) as crucial in addressing improved processes of care. Moreover, a strategic use of PMS has been found to enhance quality, compared to non-strategic use, although a clear understanding of this linkage is still to be achieved. This paper deals with the test of direct and indirect models related to the link between the strategic use of PMS and the level of improved processes in health care organizations. Indirect models were mediated by the degree of perceived managerial discretion.\nA PLS analysis on a survey of 97 Italian managers working for health care organizations in the Lombardy region was conducted. The response rate was 77.6%.\nThe strategic use of PMS in health care organizations directly and significantly (p\u2009<\u20090.001) enhances performance in terms of improved processes. Perceived managerial discretion is positively and significantly (p\u2009<\u20090.001) affected by the strategic use of PMS, whereas the mediation effect is non-significant.\nThis study contributes to the literature investigating the design and implementation of a non-financial measurement tool, such as the non-financial information included into a balanced scorecard (BSC), in health care organizations. Managers in health care organizations can benefit from the strategic use of PMS to effectively allocate their time to strategic opportunities and threats, which might arise and affect organizational, output-related performance, such as improving processes.\n", "TYPE": ["Administrative Personnel", "Female", "Health Knowledge, Attitudes, Practice", "Humans", "Italy", "Male", "Quality Indicators, Health Care", "Surveys and Questionnaires"], "FINAL_DECISION": "yes"}
{"idx": "18570208", "QUESTION": "Is severe macrosomia manifested at 11-14 weeks of gestation?", "CONCLUSION": "To determine the association between fetal biometry in the first or early second trimester and severe macrosomia at delivery.\nThis case-control study included 30 term severely macrosomic neonates; 90 appropriate-for-gestational age (AGA) neonates served as controls. All pregnancies underwent nuchal translucency (NT) screening at 11-14 weeks' gestation. Pregnancies were dated by accurate last menstrual period consistent with crown-rump length (CRL) measurements at the time of screening, early pregnancy CRL or date of fertilization. The association between birth weight and the difference between the measured and the expected CRL at the time of NT screening was analyzed.\nThe difference between measured and expected CRL, expressed both in mm and in days of gestation, was statistically greater in the severely macrosomic neonates compared with controls (mean, 6.66 +/- 4.78 mm vs. 1.17 +/- 4.6 mm, P<0.0001 and 3 +/- 2.2 days vs. 0.5 +/- 2.3 days, P<0.0001, respectively). Furthermore, there were significant correlations between the extent of macrosomia and the discrepancy between expected and measured fetal size at the time of NT screening (r = 0.47, P<0.01 and r = 0.48, P<0.01, respectively).\nSevere macrosomia apparently manifests as early as 11-14 weeks' gestation.\n", "TYPE": ["Biometry", "Case-Control Studies", "Crown-Rump Length", "Female", "Fetal Macrosomia", "Humans", "Infant, Newborn", "Nuchal Translucency Measurement", "Pregnancy", "Pregnancy Trimester, First", "Pregnancy Trimester, Second", "Retrospective Studies", "Severity of Illness Index"], "FINAL_DECISION": "yes"}
{"idx": "22117569", "QUESTION": "Is an advance care planning model feasible in community palliative care?", "CONCLUSION": "An effective advance care planning programme involves an organizational wide commitment and preparedness for health service reform to embed advance care planning into routine practice. Internationally, such programmes have been implemented predominantly in aged and acute care with more recent work in primary care.\nA multi-site action research was conducted over a 16-month period in 2007-2009 with three Victorian community palliative care services. Using mixed method data collection strategies to assess feasibility, we conducted a baseline audit of staff and clients; analysed relevant documents (client records, policies, procedures and quality improvement strategies) pre-implementation and post-implementation and conducted key informant interviews (n\u2003=\u20039).\nThree community palliative care services: one regional and two metropolitan services in Victoria, Australia.\nThe services demonstrated that it was feasible to embed the Model into their organizational structures. Advance care planning conversations and involvement of family was an important outcome measure rather than completion rate of advance care planning documents in community settings. Services adapted and applied their own concept of community, which widened the impact of the model. Changes to quality audit processes were essential to consolidate the model into routine palliative care practice.\nAn advance care planning model is feasible for community palliative care services. Quality audit processes are an essential component of the Model with documentation of advance care planning discussion established as an important outcome measure.\n", "TYPE": ["Adult", "Advance Care Planning", "Advance Directives", "Aged", "Aged, 80 and over", "Community Health Services", "Female", "Humans", "Male", "Middle Aged", "Models, Organizational", "Palliative Care", "Program Evaluation", "Rural Population", "Terminal Care", "Urban Population", "Victoria"], "FINAL_DECISION": "yes"}
{"idx": "19482903", "QUESTION": "Treadmill testing of children who have spina bifida and are ambulatory: does peak oxygen uptake reflect maximum oxygen uptake?", "CONCLUSION": "Earlier studies have demonstrated low peak oxygen uptake ((.)Vo(2)peak) in children with spina bifida. Low peak heart rate and low peak respiratory exchange ratio in these studies raised questions regarding the true maximal character of (.)Vo(2)peak values obtained with treadmill testing.\nThe aim of this study was to determine whether the Vo(2)peak measured during an incremental treadmill test is a true reflection of the maximum oxygen uptake ((.)Vo(2)max) in children who have spina bifida and are ambulatory.\nA cross-sectional design was used for this study.\nTwenty children who had spina bifida and were ambulatory participated. The (.)Vo(2)peak was measured during a graded treadmill exercise test. The validity of (.)Vo(2)peak measurements was evaluated by use of previously described guidelines for maximum exercise testing in children who are healthy, as well as differences between Vo(2)peak and (.)Vo(2) during a supramaximal protocol ((.)Vo(2)supramaximal).\nThe average values for (.)Vo(2)peak and normalized (.)Vo(2)peak were, respectively, 1.23 L/min (SD=0.6) and 34.1 mL/kg/min (SD=8.3). Fifteen children met at least 2 of the 3 previously described criteria; one child failed to meet any criteria. Although there were no significant differences between (.)Vo(2)peak and Vo(2)supramaximal, 5 children did show improvement during supramaximal testing.\nThese results apply to children who have spina bifida and are at least community ambulatory.\nThe (.)Vo(2)peak measured during an incremental treadmill test seems to reflect the true (.)Vo(2)max in children who have spina bifida and are ambulatory, validating the use of a treadmill test for these children. When confirmation of maximal effort is needed, the addition of supramaximal testing of children with disability is an easy and well-tolerated method.\n", "TYPE": ["Child", "Cross-Sectional Studies", "Exercise Test", "Female", "Heart Rate", "Humans", "Locomotion", "Male", "Mobility Limitation", "Oxygen Consumption", "Physical Endurance", "Pulmonary Ventilation", "Spinal Dysraphism", "Walking"], "FINAL_DECISION": "yes"}
{"idx": "11943048", "QUESTION": "Does receipt of hospice care in nursing homes improve the management of pain at the end of life?", "CONCLUSION": "To compare analgesic management of daily pain for dying nursing home residents enrolled and not enrolled in Medicare hospice.\nRetrospective, comparative cohort study.\nOver 800 nursing homes in Kansas, Maine, Mississippi, New York, and South Dakota.\nA subset of residents with daily pain near the end of life taken from a matched cohort of hospice (2,644) and nonhospice (7,929) nursing home residents who had at least two resident assessments (Minimum Data Sets (MDSs)) completed, their last between 1992 and 1996, and who died before April 1997. The daily pain subset consisted of 709 hospice and 1,326 nonhospice residents.\nDetailed drug use data contained on the last MDS before death were used to examine analgesic management of daily pain. Guidelines from the American Medical Directors Association (AMDA) were used to identify analgesics not recommended for use in managing chronic pain in long-term care settings. The study outcome, regular treatment of daily pain, examined whether patients received any analgesic, other than those not recommended by AMDA, at least twice a day for each day of documented daily pain (i.e., 7 days before date of last MDS).\nFifteen percent of hospice residents and 23% of nonhospice residents in daily pain received no analgesics (odds ratio (OR) = 0.57, 95% confidence interval (CI) = 0.45-0.74). A lower proportion of hospice residents (21%) than of nonhospice residents (29%) received analgesics not recommended by AMDA (OR = 0.65, 95% CI =0.52-0.80). Overall, acetaminophen (not in combination with other drugs) was used most frequently for nonhospice residents (25% of 1,673 prescriptions), whereas morphine derivatives were used most frequently for hospice residents (30% of 1,058 prescriptions). Fifty-one percent of hospice residents and 33% of nonhospice residents received regular treatment for daily pain. Controlling for clinical confounders, hospice residents were twice as likely as nonhospice residents to receive regular treatment for daily pain (adjusted odds ratio = 2.08, 95% CI = 1.68-2.56).\nFindings suggest that analgesic management of daily pain is better for nursing home residents enrolled in hospice than for those not enrolled in hospice.The prescribing practices portrayed by this study reveal that many dying nursing home residents in daily pain are receiving no analgesic treatment or are receiving analgesic treatment inconsistent with AMDA and other pain management guidelines. Improving the analgesic management of pain in nursing homes is essential if high-quality end-of-life care in nursing homes is to be achieved.\n", "TYPE": ["Aged", "Analgesics", "Cohort Studies", "Drug Utilization", "Female", "Homes for the Aged", "Hospice Care", "Humans", "Male", "Nursing Homes", "Pain", "Retrospective Studies"], "FINAL_DECISION": "yes"}
{"idx": "23347337", "QUESTION": "Is intensive chemotherapy safe for rural cancer patients?", "CONCLUSION": "To provide equality of cancer care to rural patients, Townsville Cancer Centre administers intensive chemotherapy regimens to rural patients with node-positive breast and metastatic colorectal cancers at the same doses as urban patients. Side-effects were usually managed by rural general practitioners locally.AIM: The aim is to determine the safety of this practice by comparing the profile of serious adverse events and dose intensities between urban and rural patients at the Townsville Cancer Centre.\nA retrospective audit was conducted in patients with metastatic colorectal and node-positive breast cancers during a 24-month period. Fisher's exact test was used for analysis. Rurality was determined as per rural, remote and metropolitan classification.\nOf the 121 patients included, 70 and 51 patients had breast and colon cancers respectively. The urban versus rural patient split among all patients, breast and colorectal cancer subgroups was 68 versus 53, 43 versus 27 and 25 versus 26 respectively. A total of 421 cycles was given with dose intensity of>95% for breast cancer in both groups (P>0.05). Rate of febrile neutropenia was 9.3% versus 7.4% (P = 0.56). For XELOX, rate of diarrhoea was 20% versus 19% (P = 0.66) and rate of vomiting was 20% versus 11% (P = 0.11). Only two patients were transferred to Townsville for admission. No toxic death occurred in either group.\nIt appears safe to administer intensive chemotherapy regimens at standard doses to rural patients without increased morbidity or mortality. Support for general practitioners through phone or videoconferencing may reduce the safety concerns.\n", "TYPE": ["Adult", "Aged", "Antineoplastic Agents", "Breast Neoplasms", "Colonic Neoplasms", "Diarrhea", "Female", "Humans", "Male", "Middle Aged", "Retrospective Studies", "Rural Population", "Vomiting"], "FINAL_DECISION": "yes"}
{"idx": "15489384", "QUESTION": "Does reducing spasticity translate into functional benefit?", "CONCLUSION": "Spasticity and loss of function in an affected arm are common after stroke. Although botulinum toxin is used to reduce spasticity, its functional benefits are less easily demonstrated. This paper reports an exploratory meta-analysis to investigate the relationship between reduced arm spasticity and improved arm function.\nIndividual data from stroke patients in two randomised controlled trials of intra-muscular botulinum toxin were pooled. The Modified Ashworth Scale (elbow, wrist, fingers) was used to calculate a \"Composite Spasticity Index\". Data from the arm section of the Barthel Activities of Daily Living Index (dressing, grooming, and feeding) and three subjective measures (putting arm through sleeve, cleaning palm, cutting fingernails) were summed to give a \"Composite Functional Index\". Change scores and the time of maximum change were also calculated.\nMaximum changes in both composite measures occurred concurrently in 47 patients. In 26 patients the improvement in spasticity preceded the improvement in function with 18 showing the reverse. There was a definite relationship between the maximum change in spasticity and the maximum change in arm function, independent of treatment (rho = -0.2822, p = 0.0008, n = 137). There was a clear relationship between the changes in spasticity and in arm function in patients treated with botulinum toxin (Dysport) at 500 or 1000 units (rho = -0.5679, p = 0.0090, n = 22; rho = -0.4430, p = 0.0018, n = 47), but not in those treated with placebo or 1500 units.\nUsing a targeted meta-analytic approach, it is possible to demonstrate that reducing spasticity in the arm is associated with a significant improvement in arm function.\n", "TYPE": ["Activities of Daily Living", "Aged", "Arm", "Botulinum Toxins, Type A", "Double-Blind Method", "Female", "Humans", "Injections, Intramuscular", "Male", "Middle Aged", "Motor Activity", "Muscle Spasticity", "Neuromuscular Agents", "Randomized Controlled Trials as Topic", "Stroke", "Stroke Rehabilitation"], "FINAL_DECISION": "yes"}
{"idx": "21689015", "QUESTION": "Can dogs prime autistic children for therapy?", "CONCLUSION": "Canine-assisted therapy has been receiving growing attention as a means of aiding children with autism spectrum disorder (ASD). Yet, only limited studies have been done and a great deal of literature related to this intervention is anecdotal. The present study aims at providing additional quantitative evidence on the potential of dogs to positively modulate the behavior of children with ASD.SETTINGS/\nA 12-year-old boy diagnosed with ASD was exposed, at his usual treatment location (the Portuguese Association for Developmental Disorders and Autism at Vila Nova de Gaia, Portugal), to the following treatment conditions: (1) one-to-one structured activities with a therapist assisted by a certified therapy dog, and (2) one-to-one structured activities with the same therapist alone (as a control). To accurately assess differences in the behavior of the participant between these treatment conditions, the therapist followed a strict research protocol. The behavior of the participant was continuously video-recorded during both treatment conditions for further analysis and comparison. Treatment outcomes: In the presence of the dog, the participant exhibited more frequent and longer durations of positive behaviors (such as smiling and positive physical contacting) as well as less frequent and shorter durations of negative behaviors (such as aggressive manifestations).\nThese findings are in accordance with previous experimental work and provide additional support for the assertion that dogs can prime autistic children for therapy. Ultimately, this study may contribute toward a change for full acceptance of canine-assisted therapy programs within the medical milieu. Additional studies using a similar research protocol on more autistic children will certainly help professionals to work on the most effective methods to individually serve this population through canine-assisted interventions.\n", "TYPE": ["Animal Assisted Therapy", "Animals", "Autistic Disorder", "Behavior Therapy", "Child", "Child Behavior", "Dogs", "Humans", "Male", "Portugal", "Social Behavior", "Treatment Outcome"], "FINAL_DECISION": "yes"}
{"idx": "15488260", "QUESTION": "Is the type of remission after a major depressive episode an important risk factor to relapses in a 4-year follow up?", "CONCLUSION": "Rates of relapse and predictive relapse factors were studied over more than 4 years in a sample of Spanish outpatients with DSM-III-R criteria for unipolar major depressive episode.\nA final sample of 139 outpatient was followed monthly in a naturalistic study. The Structured Clinical Interview for DSM-III-R was used. Phases of evolution were recorded using the Hamilton Depression Rating Scale, applying the Frank criteria. Survival analysis, Kaplan-Meier product limit and proportional hazards models were used.\nA higher rate of relapses was observed in the partial remission group (91.4%) compared to the complete remission one (51.3%). The four factors with predictive relapse value were: \"partial remission versus complete remission\", \"the intensity of clinical symptoms\", \"the age\" and \"the number of previous depressive episodes\". The existence of partial remission was the most powerful predictive factor.\nThe decreasing sample size during the follow-up and the difficulty in warranting the treatment compliance.\nAt medium term, relapse rates for a major depressive episode are high. Partial remission after a depressive episode seems to be an important predictive factor for relapses in a 4-year follow-up.\n", "TYPE": ["Adult", "Aged", "Antidepressive Agents", "Depressive Disorder, Major", "Diagnostic and Statistical Manual of Mental Disorders", "Drug Therapy, Combination", "Female", "Follow-Up Studies", "Humans", "Male", "Middle Aged", "Personality Inventory", "Prospective Studies", "Recurrence", "Risk Factors", "Serotonin Uptake Inhibitors", "Spain", "Survival Analysis"], "FINAL_DECISION": "yes"}
{"idx": "24298614", "QUESTION": "Is the 7th TNM edition suitable for biological predictor in early gastric cancer?", "CONCLUSION": "The clinical and prognostic value of the previous node classification of TNM staging in early gastric cancer (EGC) has been less definitive. The aim was to assess the suitability of the revised node staging for prediction of clinical behavior of EGC.\nBetween 2005 and 2008, 1,845 patients were diagnosed with EGC and underwent surgery at Severance Hospitals. Clinicopathological characteristics were analyzed with comparisons between sixth and seventh TNM staging.\nWhen comparing IB with IIA upstaged based on seventh staging, poor differentiation, signet ring cell, diffuse, undifferentiated types, perineural invasion (PNI), larger size and younger age, were more significantly associated with IIA. Clinicopathological factors were compared between N0/N1 and N2/N3 based on both staging. In mucosal cancer, younger age, diffuse and undifferentiated types were more significantly associated with N2/N3 based on seventh staging. In submucosal cancer, larger size, poor differentiation, signet ring cell, diffuse, undifferentiated types, PNI and deeper submucosal invasion, were more significantly associated with N2/N3 based on seventh staging.\nUpstaging in EGC based on the revised TNM staging reflects more aggressive biological behavior of cancer. The new TNM staging system may be informative in prediction of biological behavior of EGC as well as prognosis and survival.\n", "TYPE": ["Adult", "Aged", "Female", "Gastric Mucosa", "Humans", "Male", "Middle Aged", "Neoplasm Invasiveness", "Neoplasm Staging", "Stomach Neoplasms"], "FINAL_DECISION": "yes"}
{"idx": "25481573", "QUESTION": "Processing fluency effects: can the content and presentation of participant information sheets influence recruitment and participation for an antenatal intervention?", "CONCLUSION": "To assess the extent to which the title and font of participant information sheets (PISs) can influence pregnant women's and trainee midwives' perceptions of an antenatal intervention.\nPregnant women (n=35) and trainee midwives (n=36) were randomly presented with one of four PISs where the title and font of the PIS had been manipulated to create four experimental conditions (i.e., Double Fluent; Double Awkward; Fluent Title-Awkward Font; Awkward Title-Fluent Font). After reading the PIS, participants rated their perceptions of the intervention (i.e., Attractiveness, Complexity, Expected Risk, Required Effort) using five-point Likert scales.\nA 4\u00d72 factorial multivariate analysis of variance revealed that pregnant women rated the Double Awkward condition as significantly more complex than the Double Fluent (p=.024) and Awkward Title-Fluent Font (p=.021) conditions.\nFont influenced pregnant women's ratings of intervention complexity.\n", "TYPE": ["Adult", "Female", "Humans", "Midwifery", "Multivariate Analysis", "Pamphlets", "Patient Selection", "Pregnancy", "Pregnant Women", "Prenatal Care", "Reading"], "FINAL_DECISION": "yes"}
{"idx": "21848798", "QUESTION": "MiraLAX vs. Golytely: is there a significant difference in the adenoma detection rate?", "CONCLUSION": "In recent clinical trials (RCT) of bowel preparation, Golytely was more efficacious than MiraLAX. We hypothesised that there is a difference in adenoma detection between Golytely and MiraLAX.\nTo compare the adenoma detection rate (ADR) between these bowel preparations, and to identify independent predictors of bowel preparation quality and adenoma detection.\nThis was a post hoc analysis of an RCT that assessed efficacy and patient tolerability of Golytely vs. MiraLAX/Gatorade in average risk screening colonoscopy patients. Bowel preparation quality was measured with the Boston Bowel Preparation Scale (BBPS). An excellent/good equivalent BBPS score was defined as \u2265 7. Polyp pathology review was performed. ADR was defined as the proportion of colonoscopies with an adenoma. Univariate and multivariate analyses were conducted.\nOne hundred and ninety patients were prospectively enrolled (87 MiraLAX, 103 Golytely). Golytely had a higher rate of a BBPS score \u2265 7 (82.5% vs. MiraLAX 67.8%, P=0.02). The ADR in the Golytely cohort was 26.2% (27/103), and was 16.1% (14/87) for MiraLAX (P = 0.091). On multivariate analyses, Golytely was 2.13 \u00d7 more likely to be associated with a BBPS \u2265 7 (95% CI 1.05-4.32, P = 0.04) and 2.28 \u00d7 more likely to be associated with adenoma detection (95% CI 1.05-4.98, P = 0.04) than MiraLAX.\nGolytely was more efficacious than MiraLAX in bowel cleansing, and was independently associated with both bowel prep quality (BBPS \u2265 7) and higher adenoma detection. Golytely should be used as first line for bowel prep for colonoscopy. Studies with larger populations are needed to confirm these results.\n", "TYPE": ["Adenoma", "Age Factors", "Aged", "Colonoscopy", "Colorectal Neoplasms", "Electrolytes", "Female", "Humans", "Male", "Mass Screening", "Middle Aged", "Multivariate Analysis", "Polyethylene Glycols", "Preoperative Care", "Randomized Controlled Trials as Topic", "Retrospective Studies", "Solvents"], "FINAL_DECISION": "yes"}
{"idx": "25675614", "QUESTION": "Can gingival crevicular blood be relied upon for assessment of blood glucose level?", "CONCLUSION": "Diabetes mellitus (DM) is undiagnosed in approximately half of the patients actually suffering from the disease. In addition, the prevalence of DM is more than twice as high as in patients with periodontitis when compared to periodontally healthy subjects. Thus, a high number of patients with periodontitis may have undiagnosed DM. The purpose of the present study was to evaluate whether blood oozing from a gingival crevice during routine periodontal examination can be used for determining glucose levels.\nObservational cross-sectional studies were carried out in 75 patients (43 males and 32 females) with chronic periodontitis who were divided into two groups: Group I and Group II, respectively. Blood oozing from the gingival crevices of anterior teeth following periodontal probing was collected with the stick of glucose self-monitoring device, and the blood glucose levels were measured. At the same time, finger-prick blood was taken for glucometric analysis and subsequent readings were recorded.\nThe patient's blood glucose values ranged from 74 to 256 mg/dl. The comparison between gingival crevicular blood and finger-prick blood showed a very strong correlation, with a t value of 3.97 (at P value = 0.001).\nThe data from this study has shown that GCB collected during diagnostic periodontal examination can be an excellent source of blood for glucometric analysis.\n", "TYPE": ["Blood Glucose", "Blood Glucose Self-Monitoring", "Capillaries", "Chronic Periodontitis", "Cross-Sectional Studies", "Diabetes Mellitus", "Feasibility Studies", "Female", "Fingers", "Gingival Crevicular Fluid", "Gingival Hemorrhage", "Humans", "Male", "Reproducibility of Results"], "FINAL_DECISION": "yes"}
{"idx": "25986020", "QUESTION": "Is zero central line-associated bloodstream infection rate sustainable?", "CONCLUSION": "Adoption and implementation of evidence-based measures for catheter care leads to reductions in central line-associated bloodstream infection (CLABSI) rates in the NICU. The purpose of this study is to evaluate whether this rate reduction is sustainable for at least 1 year and to identify key determinants of this sustainability at the NICU of the Floating Hospital for Children at Tufts Medical Center.\nWe reviewed the incidence of CLABSIs in the NICU temporally to the implementation of new practice policies and procedures, from July 2008 to December 2013.\nAdoption of standardized care practices, including bundles and checklists, was associated with a significant reduction of the CLABSI rate to zero for>370 consecutive days in our NICU in 2012. Overall, our CLABSI rates decreased from 4.1 per 1000 line days in 2009 (13 infections; 3163 line days) to 0.94 in 2013 (2 infections; 2115 line days), which represents a 77% reduction over a 5-year period. In the first quarter of 2013, there was a brief increase in CLABSI rate to 3.3 per 1000 line days; after a series of interventions, the CLABSI rate was maintained at zero for>600 days. Ongoing training, surveillance, and vigilance with catheter insertion and maintenance practices and improved documentation were identified as key drivers for success.\nHigh-quality training, strict compliance with evidence-based guidelines, and thorough documentation is associated with significant reductions in CLABSIs. Mindful organizing may lead to a better understanding of what goes into a unit's ability to handle peak demands and sustain extraordinary performance in the long-term.\n", "TYPE": ["Bacteremia", "Catheter-Related Infections", "Catheterization, Central Venous", "Guideline Adherence", "Humans", "Infant, Newborn", "Time Factors"], "FINAL_DECISION": "yes"}
{"idx": "24013712", "QUESTION": "Preoperative platelet count in esophageal squamous cell carcinoma: is it a prognostic factor?", "CONCLUSION": "Platelet count is inversely related to prognosis in many cancers; however, its role in esophageal cancer is still controversial. The purpose of this study was to determine the prognostic value of preoperative platelet count in esophageal squamous cell carcinoma (ESCC).\nFrom January 2006 to December 2008, a retrospective analysis of 425 consecutive patients with ESCC was conducted. A receiver operating characteristic (ROC) curve for survival prediction was plotted to verify the optimum cutoff point for preoperative platelet count. Univariate and multivariate analyses were performed to evaluate the prognostic parameters.\nA ROC curve for survival prediction was plotted to verify the optimum cutoff point for platelet count, which was 205 (\u00d7 10(9)/L). Patients with platelet count \u2264 205 had a significantly better 5-year survival than patients with a platelet count>205 (60.7 vs. 31.6 %, P<0.001). The 5-year survival of patients either with platelet count \u2264 205 or>205 were similar (68.6 vs. 58.8 %, P = 0.085) when the nodes were negative. However, the 5-year survival of patients with platelet count \u2264 205 was better than that of patients with a platelet count>205 when the nodes were involved (32.0 vs. 12.7 %, P = 0.004). Multivariate analysis showed that platelet count (P = 0.013), T grade (P = 0.017), and N staging (P<0.001) were independent prognostic factors.\nPreoperative platelet count is a predictive factor for long-term survival in ESCC, especially in nodal-positive patients. We conclude that 205 (\u00d710(9)/L) may be the optimum cutoff point for platelet count in predicting survival in ESCC patients.\n", "TYPE": ["Carcinoma, Squamous Cell", "Esophageal Neoplasms", "Female", "Humans", "Male", "Middle Aged", "Neoplasm Staging", "Platelet Count", "Prognosis", "Retrospective Studies", "Sensitivity and Specificity", "Survival Rate"], "FINAL_DECISION": "yes"}
{"idx": "18307476", "QUESTION": "Upstream solutions: does the supplemental security income program reduce disability in the elderly?", "CONCLUSION": "The robust relationship between socioeconomic factors and health suggests that social and economic policies might substantially affect health, while other evidence suggests that medical care, the main focus of current health policy, may not be the primary determinant of population health. Income support policies are one promising avenue to improve population health. This study examines whether the federal cash transfer program to poor elderly, the Supplemental Security Income (SSI) program, affects old-age disability.\nThis study uses the 1990 and 2000 censuses, employing state and year fixed-effect models, to test whether within-state changes in maximum SSI benefits over time lead to changes in disability among people aged sixty-five and older.\nHigher benefits are linked to lower disability rates. Among all single elderly individuals, 30 percent have mobility limitations, and an increase of $100 per month in the maximum SSI benefit caused the rate of mobility limitations to fall by 0.46 percentage points. The findings were robust to sensitivity analyses. First, analyses limited to those most likely to receive SSI produced larger effects, but analyses limited to those least likely to receive SSI produced no measurable effect. Second, varying the disability measure did not meaningfully alter the findings. Third, excluding the institutionalized, immigrants, individuals living in states with exceptionally large benefit changes, and individuals living in states with no SSI supplements did not change the substantive conclusions. Fourth, Medicaid did not confound the effects. Finally, these results were robust for married individuals.\nIncome support policy may be a significant new lever for improving population health, especially that of lower-income persons. Even though the findings are robust, further analyses are needed to confirm their reliability. Future research should examine a variety of different income support policies, as well as whether a broader range of social and economic policies affect health.\n", "TYPE": ["Activities of Daily Living", "Aged", "Aged, 80 and over", "Censuses", "Disabled Persons", "Female", "Humans", "Income", "Male", "Medicaid", "Models, Econometric", "Policy Making", "Social Security", "United States"], "FINAL_DECISION": "yes"}
{"idx": "23517744", "QUESTION": "Is solitary kidney really more resistant to ischemia?", "CONCLUSION": "To our knowledge there are no evidence-based medicine data to date to critically judge the vulnerability of a solitary kidney to warm ischemia compared to paired kidneys.\nTen dogs were exposed to open right nephrectomy to create a solitary kidney model (group 1). Ten dogs with both kidneys were considered group 2. All dogs underwent warm ischemia by open occlusion of the left renal artery for 90 minutes. Dogs were sacrificed at different intervals (3 days to 4 weeks). All dogs were reevaluated by renogram before sacrifice and histopathology of the investigated kidney. The proinflammatory markers CD95 and tumor necrosis factor-\u03b1 were assessed using real-time polymerase chain reaction.\nIn group 1 clearance decreased by 20% at 1 week but basal function was regained starting at week 2. In group 2 clearance decreased more than 90% up to week 2. Recovery started at week 3 and by 4 weeks there was a 23% clearance reduction. Histopathological examination in group 1 revealed significant tubular necrosis (60%) at 3 days with regeneration starting at 1 week. In group 2 there was more pronounced tubular necrosis (90%) with regeneration starting at 2 weeks. The expression of proinflammatory markers was up-regulated in each group with higher, more sustained expression in group 2.\nSolitary kidney in a canine model is more resistant to ischemia than paired kidneys based on radiological, pathological and genetic evidence.\n", "TYPE": ["Animals", "Biopsy, Needle", "Disease Models, Animal", "Dogs", "Glomerular Filtration Rate", "Immunohistochemistry", "Ischemia", "Kidney", "Nephrectomy", "Random Allocation", "Reference Values", "Warm Ischemia"], "FINAL_DECISION": "yes"}
{"idx": "10749257", "QUESTION": "Has the mammography quality standards act affected the mammography quality in North Carolina?", "CONCLUSION": "The United States Food and Drug Administration implemented federal regulations governing mammography under the Mammography Quality Standards Act (MQSA) of 1992. During 1995, its first year in implementation, we examined the impact of the MQSA on the quality of mammography in North Carolina.\nAll mammography facilities were inspected during 1993-1994, and again in 1995. Both inspections evaluated mean glandular radiation dose, phantom image evaluation, darkroom fog, and developer temperature. Two mammography health specialists employed by the North Carolina Division of Radiation Protection performed all inspections and collected and codified data.\nThe percentage of facilities that met quality standards increased from the first inspection to the second inspection. Phantom scores passing rate was 31.6% versus 78.2%; darkroom fog passing rate was 74.3% versus 88.5%; and temperature difference passing rate was 62.4% versus 86.9%.\nIn 1995, the first year that the MQSA was in effect, there was a significant improvement in the quality of mammography in North Carolina. This improvement probably resulted from facilities' compliance with federal regulations.\n", "TYPE": ["Mammography", "North Carolina", "United States", "United States Food and Drug Administration"], "FINAL_DECISION": "yes"}
{"idx": "23916653", "QUESTION": "Orthostatic myoclonus: an underrecognized cause of unsteadiness?", "CONCLUSION": "Recently, orthostatic myoclonus (OM) has been suggested as a cause of gait impairment and unsteadiness in neurodegenerative diseases. The aim of this study was to investigate the frequency of orthostatic myoclonus, its clinical characteristics and the underlying associated neurological disorders.\nA retrospective analysis of clinical data and electromyogram surface recordings from subjects with unexplained unsteadiness/gait impairment was performed. Diagnosis of OM was made when a pattern of non-rhythmic bursts was observed (duration range 20-100 ms; bursts per second \u226416).\nAmong 93 subjects studied, OM was the most frequent disorder (n = 16; 17.2%), followed by orthostatic tremor (13.9%) and low frequency tremors during orthostatism (12.9%). All patients with OM complained about unsteadiness during orthostatism and/or during gait. Leg jerking was only observed by visual inspection during orthostatism in four subjects and two also presented falls. Eleven out of 16 patients (68.7%) with OM had an associated neurodegenerative disease, such as multiple system atrophy (n = 3) Parkinson's disease (n = 2), Alzheimer's disease (n = 2), mild cognitive impairment (n = 2) and normal pressure hydrocephalus (n = 2). Although four subjects showed improvement of orthostatic myoclonus with antimyoclonic treatment, the follow-up was not systematic enough to evaluate their therapeutic effect on OM.\nOrthostatic myoclonus is often underdiagnosed and can be considered a possible cause of unsteadiness in subjects with neurodegenerative diseases. Electromyography surface recording is thereby an aid for investigating unsteadiness of unknown origin.\n", "TYPE": ["Aged", "Aged, 80 and over", "Dizziness", "Electromyography", "Female", "Follow-Up Studies", "Gait Disorders, Neurologic", "Humans", "Male", "Middle Aged", "Myoclonus", "Neurodegenerative Diseases", "Retrospective Studies"], "FINAL_DECISION": "yes"}
{"idx": "22617083", "QUESTION": "Does age moderate the effect of personality disorder on coping style in psychiatric inpatients?", "CONCLUSION": "To examine age-related differences in the relationship between personality and coping strategies in an Australian population of psychiatric inpatients.\nConsenting eligible adults (N=238) from 18-100 years of age consecutively admitted to inpatient psychiatry units were assessed using the SCID I and II, the Coping Orientations to Problems Experienced Scale (COPE), the Brief Psychiatric Rating Scale (BPRS), the Global Assessment of Functioning Scale (GAF), the Social and Occupational Functioning Assessment Scale (SOFAS), the 12 Item Short-Form Heath Survey (SF12), the Sarason Social Support Questionnaire, and the NEO Five Factor Inventory (NEO-FFI) (cognitively impaired, and non-English speaking patients were excluded).\nOlder adults reported less symptomatology than younger patients and younger patients described more personality dysfunction than older patients. As assessed by the COPE, older adults reported lower levels of dysfunctional coping strategies than younger adults. Personality traits, social supports, gender, and age predicted coping strategies, while Axis I diagnosis, education, personality disorder, and symptom severity were not significant predictors of coping strategies.\nThis study found that influences on coping were multifactorial and moderated by age. These factors have implications for interventions designed to enhance coping strategies.\n", "TYPE": ["Adaptation, Psychological", "Adolescent", "Adult", "Age Factors", "Aged", "Aged, 80 and over", "Aging", "Australia", "Female", "Hospitals, Psychiatric", "Humans", "Male", "Middle Aged", "Multivariate Analysis", "Personality Disorders", "Prospective Studies", "Regression Analysis", "Sex Factors", "Social Support"], "FINAL_DECISION": "yes"}
{"idx": "24191126", "QUESTION": "Is CA72-4 a useful biomarker in differential diagnosis between ovarian endometrioma and epithelial ovarian cancer?", "CONCLUSION": "Surgical excision of ovarian endometriomas in patients desiring pregnancy has recently been criticized because of the risk of damage to healthy ovarian tissue and consequent reduction of ovarian reserve. A correct diagnosis in cases not scheduled for surgery is therefore mandatory in order to avoid unexpected ovarian cancer misdiagnosis. Endometriosis is often associated with high levels of CA125. This marker is therefore not useful for discriminating ovarian endometrioma from ovarian malignancy. The aim of this study was to establish if the serum marker CA72-4 could be helpful in the differential diagnosis between ovarian endometriosis and epithelial ovarian cancer.\nSerums CA125 and CA72-4 were measured in 72 patients with ovarian endometriomas and 55 patients with ovarian cancer.\nHigh CA125 concentrations were observed in patients with ovarian endometriosis and in those with ovarian cancer. A marked difference in CA72-4 values was observed between women with ovarian cancer (71.0%) and patients with endometriosis (13.8%) (P<0.0001).\nThis study suggests that CA72-4 determination can be useful to confirm the benign nature of ovarian endometriomas in women with high CA125 levels.\n", "TYPE": ["Adult", "Aged", "Antigens, Tumor-Associated, Carbohydrate", "CA-125 Antigen", "Carcinoma", "Diagnosis, Differential", "Endometriosis", "Female", "Humans", "Membrane Proteins", "Middle Aged", "Ovarian Neoplasms"], "FINAL_DECISION": "yes"}
{"idx": "20602784", "QUESTION": "Identification of racial disparities in breast cancer mortality: does scale matter?", "CONCLUSION": "This paper investigates the impact of geographic scale (census tract, zip code, and county) on the detection of disparities in breast cancer mortality among three ethnic groups in Texas (period 1995-2005). Racial disparities were quantified using both relative (RR) and absolute (RD) statistics that account for the population size and correct for unreliable rates typically observed for minority groups and smaller geographic units. Results were then correlated with socio-economic status measured by the percentage of habitants living below the poverty level.\nAfrican-American and Hispanic women generally experience higher mortality than White non-Hispanics, and these differences are especially significant in the southeast metropolitan areas and southwest border of Texas. The proportion and location of significant racial disparities however changed depending on the type of statistic (RR versus RD) and the geographic level. The largest proportion of significant results was observed for the RD statistic and census tract data. Geographic regions with significant racial disparities for African-Americans and Hispanics frequently had a poverty rate above 10.00%.\nThis study investigates both relative and absolute racial disparities in breast cancer mortality between White non-Hispanic and African-American/Hispanic women at the census tract, zip code and county levels. Analysis at the census tract level generally led to a larger proportion of geographical units experiencing significantly higher mortality rates for minority groups, although results varied depending on the use of the relative versus absolute statistics. Additional research is needed before general conclusions can be formulated regarding the choice of optimal geographic regions for the detection of racial disparities.\n", "TYPE": ["Adult", "African Americans", "Age Distribution", "Aged", "Breast Neoplasms", "Cause of Death", "Censuses", "Confidence Intervals", "Cross-Sectional Studies", "Databases, Factual", "European Continental Ancestry Group", "Female", "Health Knowledge, Attitudes, Practice", "Health Status Disparities", "Hispanic Americans", "Humans", "Incidence", "Logistic Models", "Middle Aged", "Odds Ratio", "Risk Assessment", "Socioeconomic Factors", "Survival Analysis", "Texas"], "FINAL_DECISION": "yes"}
{"idx": "18322741", "QUESTION": "Does laparoscopic surgery decrease the risk of atrial fibrillation after foregut surgery?", "CONCLUSION": "Atrial fibrillation, which occurs in 12% of all major foregut surgeries, can prolong hospital stay and increase morbidity. Minimally invasive techniques in foregut surgery have been suggested to cause less tissue trauma. We examined the factors associated with new-onset atrial fibrillation after foregut surgery at our institution.\nWe retrospectively examined the records of 154 adult patients who underwent major foregut surgery which included esophagectomy, partial or total gastrectomy, redo Heller myotomy, redo or transthoracic fundoplications. Univariate and multivariate logistic regression analysis with standard modeling techniques were performed to determine risk factors for new-onset atrial fibrillation.\nOf the 154 patients, 14 patients developed new-onset atrial fibrillation with a higher mean age of 67.1 years (+/-8.8 years) versus 56.4 years (+/-14.1 years) (p = 0.006). Laparoscopic (p = 0.004) and nonthoracic surgeries (p = 0.01) were associated with lower risk of atrial fibrillation. Patients with atrial fibrillation had received more fluid (6.5 +/- 2.8 liters versus 5.3 +/- 2.0 liters) and had longer operations (370 +/- 103 min versus 362 +/- 142 min), none of which were statistically significant. The average intensive care length of stay of patients was longer: 7.5 +/- 6.8 days versus 4.0 +/- 7.1 days (p = 0.004). Multivariate analysis revealed an association of atrial fibrillation with age (OR 1.08, 95% CI 1.02-1.14, p = 0.01), and laparoscopic surgery (OR 0.09, 95% CI 0.01-0.95, p = 0.04) after adjusting for surgery type.\nLaparoscopic surgery is associated with lower risk of atrial fibrillation in foregut surgery. Development of atrial fibrillation is associated with increased length of intensive care stay. We recommend a prospective trial to confirm our findings.\n", "TYPE": ["Adult", "Age Factors", "Aged", "Atrial Fibrillation", "Cohort Studies", "Digestive System Surgical Procedures", "Female", "Humans", "Laparoscopy", "Length of Stay", "Male", "Middle Aged", "Retrospective Studies", "Risk Factors"], "FINAL_DECISION": "yes"}
{"idx": "22348433", "QUESTION": "Does partial expander deflation exacerbate the adverse effects of radiotherapy in two-stage breast reconstruction?", "CONCLUSION": "The optimum protocol for expander volume adjustment with respect to the timing and application of radiotherapy remains controversial.\nEighteen New Zealand rabbits were divided into three groups. Metallic port integrated anatomic breast expanders of 250 cc were implanted on the back of each animal and controlled expansion was performed. Group I underwent radiotherapy with full expanders while in Group II, expanders were partially deflated immediately prior to radiotherapy. Control group did not receive radiotherapy.The changes in blood flow at different volume adjustments were investigated in Group II by laser Doppler flowmetry. Variations in the histopathologic properties of the irradiated tissues including the skin, capsule and the pocket floor, were compared in the biopsy specimens taken from different locations in each group.\nA significant increase in skin blood flow was detected in Group II with partial expander deflation. Overall, histopathologic exam revealed aggravated findings of chronic radiodermatitis (epidermal atrophy, dermal inflammation and fibrosis, neovascularisation and vascular changes as well as increased capsule thickness) especially around the lower expander pole, in Group II.\nExpander deflation immediately prior to radiotherapy, may augment the adverse effects, especially in the lower expander pole, possibly via enhanced radiosensitization due to a relative increase in the blood flow and tissue oxygenation.\n", "TYPE": ["Animals", "Breast Implants", "Breast Neoplasms", "Dermatologic Surgical Procedures", "Female", "Magnetic Resonance Imaging", "Mammaplasty", "Mastectomy", "Rabbits", "Radiation Injuries", "Radiation Oncology", "Radiotherapy Planning, Computer-Assisted", "Skin", "Tissue Expansion", "Tissue Expansion Devices"], "FINAL_DECISION": "yes"}
{"idx": "22350859", "QUESTION": "Can pictorial warning labels on cigarette packages address smoking-related health disparities?", "CONCLUSION": "The objective of this study was to determine the most effective content of pictorial health warning labels (HWLs) and whether educational attainment moderates these effects.\nField experiments were conducted with 529 adult smokers and 530 young adults (258 nonsmokers; 271 smokers). Participants reported responses to different pictorial HWLs printed on cigarette packages. One experiment involved manipulating textual form (testimonial narrative vs. didactic) and the other involved manipulating image type (diseased organs vs. human suffering).\nTests of mean ratings and rankings indicated that pictorial HWLs with didactic textual forms had equivalent or significantly higher credibility, relevance, and impact than pictorial HWLs with testimonial forms. Results from mixed-effects models confirmed these results. However, responses differed by participant educational attainment: didactic forms were consistently rated higher than testimonials among participants with higher education, whereas the difference between didactic and testimonial narrative forms was weaker or not statistically significant among participants with lower education. In the second experiment, with textual content held constant, greater credibility, relevance, and impact was found for graphic imagery of diseased organs than imagery of human suffering.\nPictorial HWLs with didactic textual forms seem to work better than those with testimonial narratives. Future research should determine which pictorial HWL content has the greatest real-world impact among consumers from disadvantaged groups, including assessment of how HWL content should change to maintain its impact as tobacco control environments strengthen and consumer awareness of smoking-related risks increases.\n", "TYPE": ["Adolescent", "Adult", "Aged", "Aged, 80 and over", "Educational Status", "Female", "Healthcare Disparities", "Humans", "Male", "Mexico", "Middle Aged", "Product Labeling", "Smoking", "Smoking Prevention", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "8916748", "QUESTION": "Do socioeconomic differences in mortality persist after retirement?", "CONCLUSION": "To assess the risk of death associated with work based and non-work based measures of socioeconomic status before and after retirement age.\nFollow up study of mortality in relation to employment grade and car ownership over 25 years.\nThe first Whitehall study.\n18,133 male civil servants aged 40-69 years who attended a screening examination between 1967 and 1970.\nDeath.\nGrade of employment was a strong predictor of mortality before retirement. For men dying at ages 40-64 the lowest employment grade had 3.12 times the mortality of the highest grade (95% confidence interval 2.4 to 4.1). After retirement the ability of grade to predict mortality declined (rate ratio 1.86; 1.6 to 2.2). A non-work based measure of socioeconomic status (car ownership) predicted mortality less well than employment grade before retirement but its ability to predict mortality declined less after retirement. Using a relative index of inequality that was sensitive to the distribution among socioeconomic groups showed employment grade and car ownership to have independent associations with mortality that were of equal magnitude after retirement. The absolute difference in death rates between the lowest and highest employment grades increased with age from 12.9 per 1000 person years at ages 40-64 to 38.3 per 1000 at ages 70-89.\nSocioeconomic differences in mortality persist beyond retirement age and in magnitude increase with age. Social differentials in mortality based on an occupational status measure seem to decrease to a greater degree after retirement than those based on a non-work measure. This suggests that alongside other socioeconomic factors work itself may play an important part in generating social inequalities in health in men of working age.\n", "TYPE": ["Adult", "Age Factors", "Aged", "Cause of Death", "England", "Follow-Up Studies", "Government", "Humans", "Male", "Middle Aged", "Mortality", "Retirement", "Social Class", "Socioeconomic Factors", "Survival Rate"], "FINAL_DECISION": "yes"}
{"idx": "18179827", "QUESTION": "Does topical ropivacaine reduce the post-tonsillectomy morbidity in pediatric patients?", "CONCLUSION": "To determine whether post-operative administration of topical ropivacaine hydrochloride decreases morbidity following adenotonsillectomy.\nProspective, randomized, double-blind clinical trial.\nUniversity referral center; ENT Department.\nFourty one children, aged 4-16 years, undergoing tonsillectomy.\nPatients received 1.0% ropivacaine hydrochloride soaked swabs packed in their tonsillar fossae while the control group received saline-soaked swabs. Mc Grath's face scale was used to compare the two groups in respect of pain control. Chi-square and two-tailed unpaired Student's t-tests or Mann-Whitney-U-tests were used to compare the two independent groups. As 10 we made 11 comparison between groups, for Bonferroni correction, p<0.005 was accepted as statistically significant.\nOnly first hour there was no significant pain-relieving effect seen in the ropivacaine group (p>0.05). The other hours and days there were statistically significance between the two groups (p<0.001). Also, the other post-operative parameters such as nausea, fever, vomiting, odor, bleeding, otalgia and trismus were not statistically different between the two groups. There were no complications associated with ropivacaine hydrochloride. No patients in this study suffered systemic side effects related to the use of this medication.\nLocally 1.0% ropivacaine administration significantly relieves the pain of pediatric tonsillectomy and, it is a safe and effective method. High concentrations of ropivaciane may produce clinically significant pain relief. It is more effective to reduce of post-operative analgesic requirement after first hour.\n", "TYPE": ["Administration, Topical", "Adolescent", "Amides", "Anesthetics, Local", "Child", "Child, Preschool", "Double-Blind Method", "Female", "Humans", "Male", "Pain Measurement", "Pain, Postoperative", "Prospective Studies", "Time Factors", "Tonsillectomy"], "FINAL_DECISION": "yes"}
{"idx": "23848044", "QUESTION": "Does oxybutynin hydrochloride cause arrhythmia in children with bladder dysfunction?", "CONCLUSION": "This study represents a subset of a complete data set, considering only those children aged admitted to the Pediatric Surgery and Pediatric Nephrology Clinics during the period January 2011 to July 2012.\nIn this study, we have determined that the QT interval changes significantly depending on the use of oxybutynin. The QT changes increased cardiac arrhythmia in children.\nFor this reason, children using such drugs should be closely monitored for cardiac arrhythmia.\n", "TYPE": ["Adolescent", "Arrhythmias, Cardiac", "Child", "Child, Preschool", "Electrocardiography", "Female", "Humans", "Male", "Mandelic Acids", "Muscarinic Antagonists", "Retrospective Studies", "Urinary Bladder, Overactive"], "FINAL_DECISION": "yes"}
{"idx": "17565137", "QUESTION": "Out of the smokescreen II: will an advertisement targeting the tobacco industry affect young people's perception of smoking in movies and their intention to smoke?", "CONCLUSION": "To evaluate the effect of an antismoking advertisement on young people's perceptions of smoking in movies and their intention to smoke.SUBJECTS/\n3091 cinema patrons aged 12-24 years in three Australian states; 18.6% of the sample (n = 575) were current smokers.DESIGN/\nQuasi-experimental study of patrons, surveyed after having viewed a movie. The control group was surveyed in week 1, and the intervention group in weeks 2 and 3. Before seeing the movie in weeks 2 and 3, a 30 s antismoking advertisement was shown, shot in the style of a movie trailer that warned patrons not to be sucked in by the smoking in the movie they were about to see.\nAttitude of current smokers and non-smokers to smoking in the movies; intention of current smokers and non-smokers to smoke in 12 months.\nAmong non-smokers, 47.8% of the intervention subjects thought that the smoking in the viewed movie was not OK compared with 43.8% of the control subjects (p = 0.04). However, there was no significant difference among smokers in the intervention (16.5%) and control (14.5%) groups (p = 0.4). A higher percentage of smokers in the intervention group indicated that they were likely to be smoking in 12 months time (38.6%) than smokers in the control group (25.6%; p<0.001). For non-smokers, there was no significant difference in smoking intentions between groups, with 1.2% of intervention subjects and 1.6% of controls saying that they would probably be smoking in 12 months time (p = 0.54).\nThis real-world study suggests that placing an antismoking advertisement before movies containing smoking scenes can help to immunise non-smokers against the influences of film stars' smoking. Caution must be exercised in the type of advertisement screened as some types of advertising may reinforce smokers' intentions to smoke.\n", "TYPE": ["Adolescent", "Advertising as Topic", "Attitude to Health", "Female", "Humans", "Male", "Motion Pictures", "Smoking", "Smoking Prevention"], "FINAL_DECISION": "yes"}
{"idx": "25604390", "QUESTION": "Aberrant loss of dickkopf-3 in gastric cancer: can it predict lymph node metastasis preoperatively?", "CONCLUSION": "Dickkopf-3 (DKK3) may act as a tumor suppressor as it is down-regulated in various types of cancer. This study assessed the DKK3 protein expression in gastric cancer and its potential value as a prognostic marker.\nDKK3 expression was evaluated by immunohistochemistry in 158 gastric cancer samples from patients who underwent gastrectomy from 2002 to 2008. Clinicopathological parameters and survival data were analyzed.\nLoss of DKK3 expression was found in 64 of 158 (40.5%) samples, and it was associated with advanced T stage (p<0.001), lymph node metastasis (p<0.001), UICC TNM stage (p<0.001), tumor location (p = 0.029), lymphovascular invasion (p = 0.035), and perineural invasion (p = 0.032). Patients without DKK3 expression in tumor cells had a significantly worse disease-free and overall survival than those with DKK3 expression (p<0.001, and p = 0.001, respectively). TNM stage (p = 0.028 and p<0.001, respectively) and residual tumor (p<0.001 and p = 0.003, respectively) were independent predictors of disease-free and overall survival. Based on the preoperative clinical stage assessed by computed tomography (CT), loss of DKK3 expression was predominantly associated with worse prognosis in patients with clinically node-negative advanced gastric cancer (AGC). The combination of DKK3 expression status and CT increased the accuracy of CT staging for predicting lymph node involvement from 71.5 to 80.0% in AGC patients.\nLoss of DKK3 protein expression was significantly associated with poor survival in patients with gastric cancer and was strongly correlated with the TNM stage. DKK3 might be a potential biomarker of lymph node involvement that can improve the predictive power of CT.\n", "TYPE": ["Adenocarcinoma", "Aged", "Disease-Free Survival", "Female", "Gastrectomy", "Humans", "Immunohistochemistry", "Intercellular Signaling Peptides and Proteins", "Lymphatic Metastasis", "Male", "Middle Aged", "Neoplasm Invasiveness", "Neoplasm Staging", "Neoplasm, Residual", "Preoperative Period", "Retrospective Studies", "Stomach Neoplasms", "Survival Rate", "Tomography, X-Ray Computed"], "FINAL_DECISION": "yes"}
{"idx": "14968373", "QUESTION": "Can CT predict the level of CSF block in tuberculous hydrocephalus?", "CONCLUSION": "Treatment of obstructive hydrocephalus in children with tuberculous meningitis (TBM) depends on the level of the cerebrospinal fluid (CSF) block. Air-encephalography is regarded as the gold standard for differentiating communicating and non-communicating hydrocephalus. Since air-encephalography involves a lumbar puncture, it carries the risk of cerebral herniation. AIM. The aim of this study was to determine whether communicating and non-communicating hydrocephalus in TBM can be differentiated by means of cranial computerised tomography (CT).\nA number of CT indices were measured in 50 children with communicating and 34 children with non-communicating hydrocephalus according to air-encephalographic findings.\nThe only CT finding that correlated with the type of hydrocephalus was the shape of the third ventricle. Significantly more children with non-communicating hydrocephalus had a rounded third ventricle than those with communicating hydrocephalus.\nCT is therefore not useful in determining the level of CSF block in TBM. Air-encephalography remains the most reliable way of determining the level of CSF obstruction.\n", "TYPE": ["Cerebrospinal Fluid", "Child, Preschool", "Diagnosis, Differential", "Female", "Humans", "Hydrocephalus", "Infant", "Male", "Pneumoencephalography", "Radiographic Image Interpretation, Computer-Assisted", "Retrospective Studies", "Sensitivity and Specificity", "Third Ventricle", "Tomography, X-Ray Computed", "Tuberculosis, Meningeal"], "FINAL_DECISION": "yes"}
{"idx": "10135926", "QUESTION": "Is oral endotracheal intubation efficacy impaired in the helicopter environment?", "CONCLUSION": "Patients transported by helicopter often require advanced airway management. The purpose of this study was to determine whether or not the in-flight environment of air medical transport in a BO-105 helicopter impairs the ability of flight nurses to perform oral endotracheal intubation.\nThe study was conducted in an MBB BO-105 helicopter.\nFlight nurses performed three manikin intubations in each of the two study environments: on an emergency department stretcher and in-flight in the BO-105 helicopter.\nThe mean time required for in-flight intubation (25.9 +/- 10.9 seconds) was significantly longer than the corresponding time (13.2 +/- 2.8 seconds) required for intubation in the control setting (ANOVA, F = 38.7, p<.001). All intubations performed in the control setting were placed correctly in the trachea; there were two (6.7%) esophageal intubations in the in-flight setting. The difference in appropriate endotracheal intubation between the two settings was not significant (chi 2 = 0.3; p>0.05).\nOral endotracheal intubation in the in-flight setting of the BO-105 helicopter takes approximately twice as long as intubation in a ground setting. The results support pre-flight intubation of patients who appear likely to require urgent intubation during air medical transport in the BO-105 helicopter.\n", "TYPE": ["Air Ambulances", "Analysis of Variance", "Data Collection", "Emergency Nursing", "Humans", "Intubation, Intratracheal", "North Carolina", "Time and Motion Studies", "Transportation of Patients", "Treatment Outcome"], "FINAL_DECISION": "yes"}
{"idx": "23379759", "QUESTION": "Can early second-look tympanoplasty reduce the rate of conversion to modified radical mastoidectomy?", "CONCLUSION": "The aims of the study were to report the rates of recurrent and residual cholesteatoma following primary CAT surgery and to report the rate of conversion to a modified radical mastoidectomy.\nThis was a retrospective review of a single surgeon series between 2006 and 2012.\nIn total 132 second-look operations were undertaken, with a mean interval between primary surgery and second-look procedures of 6 months. The rate of cholesteatoma at second-look surgery was 19.7%, which was split into residual disease (10.6%) and recurrent disease (9.09%). New tympanic membrane defects with cholesteatoma were considered as recurrent disease. Residual disease was defined as cholesteatoma present behind an intact tympanic membrane. The majority of recurrent and residual disease was easily removed at second look (73.1%). Only four cases were converted to a modified radical mastoidectomy (3%) and three cases required a third-look procedure.\nCombined approach tympanoplasty (CAT) allows for successful treatment of cholesteatoma with rates of recurrent and residual disease comparable to open mastoid surgery. Early timing of second-look procedures allows easier removal of any recurrent or residual disease, which reduces the conversion rate to open mastoidectomy.\n", "TYPE": ["Adolescent", "Adult", "Child", "Child, Preschool", "Cholesteatoma, Middle Ear", "Humans", "Middle Aged", "Recurrence", "Retrospective Studies", "Second-Look Surgery", "Treatment Outcome", "Tympanoplasty", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "19923859", "QUESTION": "Can T-cell deficiency affect spatial learning ability following toluene exposure?", "CONCLUSION": "The present studywas designed to investigate the possible role of T cells in spatial learning ability in mouse after toluene exposure.\nEight-week-old male wild-type (WT) and nude mice of BALB/c strain were exposed to toluene (0, 9 and 90 ppm) in a nose-only exposure chamber for 30 min per day for 3 consecutive days and then once per week for 4 weeks. Twenty-four hours after the completion of exposure, we examined the spatial learning ability in each mouse using the Morris water maze apparatus.\nIn the acquisition phase, a longer escape latency was observed in nude mice exposed to 90 ppm toluene on days 3 and 4 when compared with corresponding WT mice. However, the effect of toluene on the escape latency was not significant in nude mice. In the probe trial, WT mice exposed to 90 ppm toluene showed poor retention memory compared with the control group. In the reversal phase, we did not find any significant difference between groups.\nThese results indicate that T-cell deficiency may affect spatial learning performance following toluene exposure.\n", "TYPE": ["Animals", "Astrocytes", "Disease Models, Animal", "Immune Tolerance", "Immunity, Cellular", "Immunity, Innate", "Learning Disorders", "Male", "Maze Learning", "Memory Disorders", "Mice", "Mice, Inbred BALB C", "Microglia", "Neuroimmunomodulation", "Neurotoxins", "Solvents", "T-Lymphocytes", "Toluene"], "FINAL_DECISION": "yes"}
{"idx": "22656647", "QUESTION": "Are acceptance rates of a national preventive home visit programme for older people socially imbalanced?", "CONCLUSION": "Preventive home visits are offered to community dwelling older people in Denmark aimed at maintaining their functional ability for as long as possible, but only two thirds of older people accept the offer from the municipalities. The purpose of this study is to investigate 1) whether socioeconomic status was associated with acceptance of preventive home visits among older people and 2) whether municipality invitational procedures for the preventive home visits modified the association.\nThe study population included 1,023 community dwelling 80-year-old individuals from the Danish intervention study on preventive home visits. Information on preventive home visit acceptance rates was obtained from questionnaires. Socioeconomic status was measured by financial assets obtained from national registry data, and invitational procedures were identified through the municipalities. Logistic regression analyses were used, adjusted by gender.\nOlder persons with high financial assets accepted preventive home visits more frequently than persons with low assets (adjusted OR = 1.5 (CI95%: 1.1-2.0)). However, the association was attenuated when adjusted by the invitational procedures. The odds ratio for accepting preventive home visits was larger among persons with low financial assets invited by a letter with a proposed date than among persons with high financial assets invited by other procedures, though these estimates had wide confidence intervals.\nHigh socioeconomic status was associated with a higher acceptance rate of preventive home visits, but the association was attenuated by invitational procedures. The results indicate that the social inequality in acceptance of publicly offered preventive services might decrease if municipalities adopt more proactive invitational procedures.\n", "TYPE": ["Aged, 80 and over", "Cross-Sectional Studies", "Denmark", "Female", "Financing, Personal", "Geriatric Assessment", "Health Services for the Aged", "Healthcare Disparities", "Home Care Services", "House Calls", "Humans", "Logistic Models", "Male", "Patient Acceptance of Health Care", "Physicians, Family", "Prevalence", "Preventive Health Services", "Program Evaluation", "Residence Characteristics", "Sex Distribution", "Social Class", "Surveys and Questionnaires"], "FINAL_DECISION": "yes"}
{"idx": "21658267", "QUESTION": "Do improvements in outreach, clinical, and family and community-based services predict improvements in child survival?", "CONCLUSION": "There are three main service delivery channels: clinical services, outreach, and family and community. To determine which delivery channels are associated with the greatest reductions in under-5 mortality rates (U5MR), we used data from sequential population-based surveys to examine the correlation between changes in coverage of clinical, outreach, and family and community services and in U5MR for 27 high-burden countries.\nHousehold survey data were abstracted from serial surveys in 27 countries. Average annual changes (AAC) between the most recent and penultimate survey were calculated for under-five mortality rates and for 22 variables in the domains of clinical, outreach, and family- and community-based services. For all 27 countries and a subset of 19 African countries, we conducted principal component analysis to reduce the variables into a few components in each domain and applied linear regression to assess the correlation between changes in the principal components and changes in under-five mortality rates after controlling for multiple potential confounding factors.\nAAC in under 5-mortality varied from 6.6% in Nepal to -0.9% in Kenya, with six of the 19 African countries all experiencing less than a 1% decline in mortality. The strongest correlation with reductions in U5MR was observed for access to clinical services (all countries: p = 0.02, r\u00b2 = 0.58; 19 African countries p<0.001, r\u00b2 = 0.67). For outreach activities, AAC U5MR was significantly correlated with antenatal care and family planning services, while AAC in immunization services showed no association. In the family- and community services domain, improvements in breastfeeding were associated with significant changes in mortality in the 30 countries but not in the African subset; while in the African countries, nutritional status improvements were associated with a significant decline in mortality.\nOur findings support the importance of increasing access to clinical services, certain outreach services and breastfeeding and, in Africa, of improving nutritional status. Integrated programs that emphasize these services may lead to substantial mortality declines.\n", "TYPE": ["Adolescent", "Adult", "Africa", "Asia", "Caribbean Region", "Child Health Services", "Child Mortality", "Child, Preschool", "Community Health Services", "Community-Institutional Relations", "Cross-Sectional Studies", "Family", "Female", "Forecasting", "Humans", "Infant", "Latin America", "Male", "Middle Aged", "Middle East", "Survival", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "23375036", "QUESTION": "An HIV1/2 point of care test on sputum for screening TB/HIV co-infection in Central India - Will it work?", "CONCLUSION": "To determine whether the OraQuick\u00ae HIV-1/2 Assay (OraSure Technologies, Inc., Bethlehem, PA, USA) in sputum is a valid tool for HIV surveillance among TB patients.\nA cross sectional study was carried out on sputa of patients diagnosed with tuberculosis. Sputa were tested for antibodies to HIV using OraQuick\u00ae HIV-1/2 Assay (OraSure Technologies, Inc., Bethlehem, PA, USA). The results were compared with results of serum ELISA.\nCompared to serum ELISA, the OraQuick\u00ae HIV-1/2 Assay in sputum specimens reported 90% sensitivity (9/10) and 100% specificity (307/307), with a positive predictive value of 100% (95%CI: 66.37%-100.00%) and a negative predictive value of 99.68% (95%CI: 98.20%-99.99%).\nThis testing method may provide a useful strategy for conducting HIV surveillance in possible co-infected TB patients at peripheral centres. Since there is no investment on infrastructure, it may be possible for paramedical health professionals to carry out the test, particularly in areas with low HIV endemicity.\n", "TYPE": ["Adolescent", "Adult", "Aged", "Cross-Sectional Studies", "Enzyme-Linked Immunosorbent Assay", "Female", "HIV Infections", "HIV-1", "HIV-2", "Humans", "India", "Male", "Mass Screening", "Middle Aged", "Point-of-Care Systems", "Sensitivity and Specificity", "Sputum", "Tuberculosis, Pulmonary", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "29112560", "QUESTION": "Is the Distance Worth It?", "CONCLUSION": "It is unclear whether traveling long distances to high-volume centers would compensate for travel burden among patients undergoing rectal cancer resection.\nThe purpose of this study was to determine whether operative volume outweighs the advantages of being treated locally by comparing the outcomes of patients with rectal cancer treated at local, low-volume centers versus far, high-volume centers.\nThis was a population-based study.\nThe National Cancer Database was queried for patients with rectal cancer.\nPatients with stage II or III rectal cancer who underwent surgical resection between 2006 and 2012 were included.\nThe outcomes of interest were margins, lymph node yield, receipt of neoadjuvant chemoradiation, adjuvant chemotherapy, readmission within 30 days, 30-day and 90-day mortality, and 5-year overall survival.\nA total of 18,605 patients met inclusion criteria; 2067 patients were in the long-distance/high-volume group and 1362 in the short-distance/low-volume group. The median travel distance was 62.6 miles for the long-distance/high-volume group and 2.3 miles for the short-distance/low-volume group. Patients who were younger, white, privately insured, and stage III were more likely to have traveled to a high-volume center. When controlled for patient factors, stage, and hospital factors, patients in the short-distance/low-volume group had lower odds of a lymph node yield \u226512 (OR = 0.51) and neoadjuvant chemoradiation (OR = 0.67) and higher 30-day (OR = 3.38) and 90-day mortality (OR = 2.07) compared with those in the long-distance/high-volume group. The short-distance/low-volume group had a 34% high risk of overall mortality at 5 years compared with the long-distance/high-volume group.\nWe lacked data regarding patient and physician decision making and surgeon-specific factors.\nOur results indicate that when controlled for patient, tumor, and hospital factors, patients who traveled a long distance to a high-volume center had improved lymph node yield, neoadjuvant chemoradiation receipt, and 30- and 90-day mortality compared with those who traveled a short distance to a low-volume center. They also had improved 5-year survival. See Video Abstract at http://links.lww.com/DCR/A446.\n", "TYPE": ["Adenocarcinoma", "Adenocarcinoma, Mucinous", "Aged", "Chemoradiotherapy", "Chemotherapy, Adjuvant", "Female", "Health Services Accessibility", "Hospitals, High-Volume", "Humans", "Lymph Node Excision", "Male", "Margins of Excision", "Middle Aged", "Neoplasm Staging", "Patient Readmission", "Rectal Neoplasms", "Risk Factors", "Survival Rate", "Travel", "Treatment Outcome", "United States"], "FINAL_DECISION": "yes"}
{"idx": "21420186", "QUESTION": "Could ADMA levels in young adults born preterm predict an early endothelial dysfunction?", "CONCLUSION": "Sporadic data present in literature report how preterm birth and low birth weight are risk factors for the development of cardiovascular diseases in later life. High levels of asymmetric dimethylarginine (ADMA), a strong inhibitor of nitric oxide synthesis, are associated with the future development of adverse cardiovascular events and cardiac death.\n1) to verify the presence of a statistically significant difference between ADMA levels in young adults born preterm at extremely low birth weight (<1000 g; ex-ELBW) and those of a control group of healthy adults born at term (C) and 2) to seek correlations between ADMA levels in ex-ELBW and anthropometric and clinical parameters (gender, chronological age, gestational age, birth weight, and duration of stay in Neonatal Intensive Care Unit).\nThirty-two ex-ELBW subjects (11 males [M] and 21 females [F], aged 17-29years, mean age 22.2 \u00b1 2.3 years) were compared with 25 C (7 M and 18F). ADMA levels were assessed by high-performance liquid chromatography with highly sensitive laser fluorescent detection.\nADMA levels were reduced in ex-ELBW subjects compared to C (0.606+0.095 vs 0.562+0.101 \u03bcmol/L, p<0.05), and significantly correlated inversely with gestational age (r=-0.61, p<0.00001) and birth weight (r=-0.57, p<0.0002).\nOur findings reveal a significant decrease in ADMA levels of ex-ELBW subjects compared to C, underlining a probable correlation with preterm birth and low birth weight. Taken together, these results may underlie the onset of early circulatory dysfunction predictive of increased cardiovascular risk.\n", "TYPE": ["Adolescent", "Adult", "Arginine", "Early Diagnosis", "Endothelium, Vascular", "Female", "Gestational Age", "Humans", "Infant, Extremely Low Birth Weight", "Infant, Low Birth Weight", "Infant, Newborn", "Male", "Predictive Value of Tests", "Premature Birth", "Vascular Diseases", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "25521278", "QUESTION": "Is plate clearing a risk factor for obesity?", "CONCLUSION": "Identifying eating behaviors which contribute to excess weight gain will inform obesity prevention strategies. A tendency to clear one's plate when eating may be a risk factor for obesity in an environment where food is plentiful. Whether plate clearing is associated with increased body weight in a cohort of US participants was examined.\nNine hundred and ninety-three US adults (60% male, 80% American European, mean age=31 years) completed self-report measures of habitual plate clearing together with behavioral and demographic characteristics known to be associated with obesity.\nPlate clearing tendencies were positively associated with BMI and remained so after accounting for a large number of other demographic and behavioral predictors of BMI in analyses (\u03b2=0.18, 95% CIs=0.07, 0.29, P<0.001); an increased tendency to plate clear was associated with a significantly higher body weight.\nThe tendency to clear one's plate when eating is associated with increased body weight and may constitute a risk factor for weight gain.\n", "TYPE": ["Adolescent", "Adult", "Aged", "Body Mass Index", "Body Weight", "Cross-Sectional Studies", "Feeding Behavior", "Female", "Health Surveys", "Humans", "Male", "Middle Aged", "Obesity", "Risk Factors", "Self Report", "Surveys and Questionnaires", "United States", "Weight Gain", "Young Adult"], "FINAL_DECISION": "yes"}
{"idx": "23283159", "QUESTION": "Is obesity a risk factor for wheezing among adolescents?", "CONCLUSION": "To investigate the effect of obesity at the start of adolescence on the prevalence, incidence and maintenance of chest wheezing among individuals aged 11-15 years in a birth cohort in a developing country.\nThe seventh follow-up of the 1993 Pelotas birth cohort occurred in 2004 (individuals aged 10-11 years). Between January and August 2008, the eighth follow-up of the cohort was conducted. All the individuals of the original cohort who were alive (who were then adolescents aged between 14 and 15 years) were targets for the study. The International Study of Asthma and Allergies in Childhood (ISAAC) questionnaire was used to define wheezing. In addition to the body mass index (BMI), used to define obesity by the World Health Organization (WHO) criteria, we assessed skinfold thickness.\nFrom the original cohort, 4,349 individuals were located (85.7% follow-up rate). The prevalence of chest wheezing at 11 and 15 years were 13.5% (95% CI: 12.5%-14.5%) and 12.1% (95% CI: 11.1%-13.1%), respectively. The prevalence of wheezing at both times was 4.5% (95% CI: 3.9%-5.1%) and the incidence of wheezing was 7.5% (95% CI: 6.7%-8.3%). Independent of the effect of various confounding variables, the prevalence of wheezing at 15 years was 50% greater among obese individuals than among eutrophic individuals at 11 years (RR 1.53; 95% CI: 1.14-2.05). The greater the skinfold tertile at 11 years, the higher the prevalence of wheezing at 15 years was (p = .011). Weight status and skinfolds did not present any association with incident wheezing. After controlling for confounding factors, the risk of persistent wheezing among obese individuals at 11 years was 1.82 (95% CI: 1.30-2.54).\nSince obesity at the start of adolescence is associated with asthma symptom persistence, prevention and treatment of obesity may reduce avoidable healthcare costs and disease burden.\n", "TYPE": ["Adolescent", "Asthma", "Brazil", "Child", "Cohort Studies", "Confounding Factors (Epidemiology)", "Female", "Humans", "Incidence", "Male", "Obesity", "Prevalence", "Respiratory Sounds", "Risk Factors"], "FINAL_DECISION": "yes"}
{"idx": "12848629", "QUESTION": "Is a 9-month treatment sufficient in tuberculous enterocolitis?", "CONCLUSION": "Tuberculosis has increased in parallel with the acquired immunodeficiency syndrome epidemic and the use of immunosuppressive therapy, and the growing incidence of extra-pulmonary tuberculosis, especially with intestinal involvement, reflects this trend. However, the duration of anti-tuberculous therapy has not been clarified in intestinal tuberculosis.AIM: To compare the efficacy of different treatment durations in tuberculous enterocolitis in terms of response and recurrence rates.\nForty patients with tuberculous enterocolitis were randomized prospectively: 22 patients into a 9-month and 18 into a 15-month group. Diagnosis was made either by colonoscopic findings of discrete ulcers and histopathological findings of caseating granuloma and/or acid-fast bacilli, or by clinical improvement after therapeutic trial. Patients were followed up with colonoscopy every other month until complete response or treatment completion, and then every 6 months for 1 year and annually. Complete response was defined as a resolution of symptoms and active tuberculosis by colonoscopy.\nComplete response was obtained in all patients in both groups. Two patients in the 9-month group and one in the 15-month group underwent operation due to intestinal obstruction and perianal fistula, respectively. No recurrence of active intestinal tuberculosis occurred during the follow-up period in either group.\nTuberculous enterocolitis can be managed by 9-month chemotherapy without disease recurrence. Further investigations are needed in immunocompromised patients.\n", "TYPE": ["Adult", "Antitubercular Agents", "Colonoscopy", "Female", "Follow-Up Studies", "Humans", "Male", "Prospective Studies", "Recurrence", "Treatment Outcome", "Tuberculosis, Gastrointestinal"], "FINAL_DECISION": "yes"}
{"idx": "18222909", "QUESTION": "Are pectins involved in cold acclimation and de-acclimation of winter oil-seed rape plants?", "CONCLUSION": "The hypothesis was tested that pectin content and methylation degree participate in regulation of cell wall mechanical properties and in this way may affect tissue growth and freezing resistance over the course of plant cold acclimation and de-acclimation.\nExperiments were carried on the leaves of two double-haploid lines of winter oil-seed rape (Brassica napus subsp. oleifera), differing in winter survival and resistance to blackleg fungus (Leptosphaeria maculans).\nPlant acclimation in the cold (2 degrees C) brought about retardation of leaf expansion, concomitant with development of freezing resistance. These effects were associated with the increases in leaf tensile stiffness, cell wall and pectin contents, pectin methylesterase (EC 3.1.1.11) activity and the low-methylated pectin content, independently of the genotype studied. However, the cold-induced modifications in the cell wall properties were more pronounced in the leaves of the more pathogen-resistant genotype. De-acclimation promoted leaf expansion and reversed most of the cold-induced effects, with the exception of pectin methylesterase activity.\nThe results show that the temperature-dependent modifications in pectin content and their methyl esterification degree correlate with changes in tensile strength of a leaf tissue, and in this way affect leaf expansion ability and its resistance to freezing and to fungus pathogens.\n", "TYPE": ["Acclimatization", "Ascomycota", "Biomechanical Phenomena", "Brassica napus", "Carboxylic Ester Hydrolases", "Cell Enlargement", "Cell Wall", "Esterification", "Freezing", "Genotype", "Pectins", "Plant Diseases", "Plant Leaves"], "FINAL_DECISION": "yes"}
{"idx": "24014276", "QUESTION": "Optimism and survival: does an optimistic outlook predict better survival at advanced ages?", "CONCLUSION": "Studies examining predictors of survival among the oldest-old have primarily focused on objective measures, such as physical function and health status. Only a few studies have examined the effect of personality traits on survival, such as optimism. The aim of this study was to examine whether an optimistic outlook predicts survival among the oldest-old.\nThe Danish 1905 Cohort Survey is a nationwide, longitudinal survey comprising all individuals born in Denmark in 1905. At baseline in 1998, a total of 2,262 persons aged 92 or 93 agreed to participate in the intake survey. The baseline in-person interview consisted of a comprehensive questionnaire including physical functioning and health, and a question about whether the respondent had an optimistic, neutral or pessimistic outlook on his or her own future.\nDuring the follow-up period of 12 years (1998-2010) there were 2,239 deaths (99 %) in the 1905 Cohort Survey. Univariable analyses revealed that optimistic women and men were at lower risk of death compared to their neutral counterparts [HR 0.82, 95 % CI (0.73-0.93) and 0.81, 95 % CI (0.66-0.99), respectively]. When confounding factors such as baseline physical and cognitive functioning and disease were taken into account the association between optimism and survival weakened in both sexes, but the general pattern persisted. Optimistic women were still at lower risk of death compared to neutral women [HR 0.85, 95 % CI (0.74-0.97)]. The risk of death was also decreased for optimistic men compared to their neutral counterparts, but the effect was non-significant [HR 0.91, 95 % CI (0.73-1.13)].\nAn optimistic outlook appears to be a significant predictor of survival among the oldest-old women. It may also be a significant predictor for men but the sample size is small.\n", "TYPE": ["Aged", "Denmark", "Female", "Follow-Up Studies", "Health Status", "Humans", "Longitudinal Studies", "Male", "Survival"], "FINAL_DECISION": "yes"}
{"idx": "12172698", "QUESTION": "Is withdrawal-induced anxiety in alcoholism based on beta-endorphin deficiency?", "CONCLUSION": "Associations between several psychopathological alterations and lowered beta-endorphin(beta E) plasma levels have already been stated in former studies. However, whereas single measures during static conditions generally failed in linking beta E levels with psychopathology, dynamic changes of beta E in particular have been shown to be associated with spells of anxiety and depression. During alcohol withdrawal, a decreased secretion of beta E with a delayed normalization has been reported, but up to now only few data became available regarding the interaction of plasma beta E and psychopathological parameters.\nThe aim of our study was to test the hypothesis whether beta E during acute alcohol withdrawal is associated with anxiety, depression, and craving.\nWe observed self-rated anxiety, depression, and craving during alcohol withdrawal and assessed beta E levels (RIA) in a consecutive sample of 60 alcoholics on day 1 and day 14 after onset of withdrawal, and in 30 healthy volunteers. To control for mutual interactions of beta E and the pituitary-adrenocortical hormone secretion, plasma corticotropin (ACTH) and cortisol were also determined.\nIn accordance with prior studies, beta E was significantly lowered on day 1 and day 14 of alcohol withdrawal relative to controls. Plasma levels of ACTH correlated significantly with beta E in alcoholics at both time points and in controls, without differing significantly between the groups. Self-rated anxiety, depression, and alcohol craving decreased significantly between day 1 and day 14. Levels of beta E were inversely correlated with anxiety day 1 (r=-0.58) and day 14 (r=-0.71). Partial correlation coefficients controlling for ACTH plasma levels revealed that this correlation was largely independent from ACTH. In addition, a significant inverse relationship was found between beta E and craving on day 14 (r=-0.28). No association appeared between beta E and depression.\nOur results give first evidence that lowered beta E during alcohol withdrawal may contribute to anxiety as a common disturbance during this state.\n", "TYPE": ["Adrenocorticotropic Hormone", "Adult", "Alcohol Drinking", "Alcoholism", "Anxiety", "Depression", "Disruptive, Impulse Control, and Conduct Disorders", "Female", "Humans", "Hydrocortisone", "Male", "Middle Aged", "Substance Withdrawal Syndrome", "Time Factors", "beta-Endorphin"], "FINAL_DECISION": "yes"}
{"idx": "12419743", "QUESTION": "Is first-line single-agent mitoxantrone in the treatment of high-risk metastatic breast cancer patients as effective as combination chemotherapy?", "CONCLUSION": "To determine whether patients with high-risk metastatic breast cancer draw benefit from combination chemotherapy as first-line treatment.\nA total of 260 women with measurable metastatic breast cancer fulfilling high-risk criteria, previously untreated with chemotherapy for their metastatic disease, were randomized to receive either mitoxantrone 12 mg/m(2) or the combination of fluorouracil 500 mg/m(2), epirubicin 50 mg/m(2) and cyclophosphamide 500 mg/m(2) (FEC) every 3 weeks. Treatment was continued until complete remission plus two cycles, or until disease progression. In the case of partial remission or stable disease, treatment was stopped after 12 cycles. Second-line treatment was vindesine, mitomycin and prednisolone. Gain from treatment was estimated using a modified Brunner's score composed of time to progression, patients' rating of the treatment benefit, alopecia, vomiting and performance status.\nAfter recruitment from 1992 to 1997 and observation from 1997 to 1999, the final evaluation showed that single-agent treatment with mitoxantrone does not differ significantly from combination treatment with FEC in terms of response, objective remission rate, remission duration, time to response, time to best response, time to progression or overall survival. There was, however, a significant difference in gain from treatment using a modified Brunner's score favoring the single-agent treatment arm. There was no evidence that any subgroup would fare better with combination treatment.\nNo significant difference was detected between the treatment with mitoxantrone as a single agent and the combination of low-dose FEC in terms of response or survival; therefore, the imperative of the necessity of first-line combination chemotherapy for patients with high-risk metastatic breast cancer may be questioned. Since toxicity and quality of life score favored the single-agent mitoxantrone treatment arm, this treatment may be offered to patients preferring quality of life to a potential small prolongation of survival.\n", "TYPE": ["Adult", "Aged", "Antineoplastic Combined Chemotherapy Protocols", "Biopsy, Needle", "Bone Neoplasms", "Breast Neoplasms", "Cyclophosphamide", "Disease-Free Survival", "Epirubicin", "Female", "Fluorouracil", "Germany", "Humans", "Liver Neoplasms", "Logistic Models", "Lung Neoplasms", "Middle Aged", "Mitoxantrone", "Neoplasm Staging", "Probability", "Proportional Hazards Models", "Quality of Life", "Risk Assessment", "Sensitivity and Specificity", "Statistics, Nonparametric", "Survival Analysis", "Treatment Outcome"], "FINAL_DECISION": "yes"}
{"idx": "24476003", "QUESTION": "Is nasogastric decompression useful in prevention of leaks after laparoscopic sleeve gastrectomy?", "CONCLUSION": "Although its excellent results, laparoscopic sleeve gastrectomy (LSG) presents major complications ranging from 0% to 29%. Among them, the staple line leak presents an incidence varying from 0% to 7%. Many trials debated about different solutions in order to reduce leaks' incidence. No author has investigated the role of gastric decompression in the prevention of this complication. Aim of our work is to evaluate if this procedure can play a role in avoiding the occurrence of staple line leaks after LSG.\nBetween January 2008 and November 2012, 145 patients were prospectively and randomly included in the study. Seventy patients composed the group A, whose operations were completed with placement of nasogastric tube; the other 75 patients were included in the group B, in which no nasogastric tube was placed.\nNo statistical differences were observed between group A and group B regarding gender distribution, age, weight, and BMI. No intraoperative complications and no conversion occurred in both groups. Intraoperative blood loss (50.1 \u00b1 42.3 vs. 52.5 \u00b1 37.6 ml, respectively) and operative time (65.4 \u00b1 25.5 vs. 62.6 \u00b1 27.8 min, respectively) were comparable between the two groups (p: NS). One staple line leak (1.4%) occurred on 6th postoperative day in group A patients. No leak was observed in group B patients. Postoperative hospital stay was significantly longer in group A vs. group B patients (7.6 \u00b1 3.4 vs. 6.2 \u00b1 3.1 days, respectively, p: 0.04).\nRoutine placement of nasogastric tube in patients operated of LSG seems not useful in reducing leaks' incidence.\n", "TYPE": ["Adult", "Bariatric Surgery", "Female", "Gastrectomy", "Humans", "Intubation, Gastrointestinal", "Male", "Middle Aged", "Obesity, Morbid", "Postoperative Complications", "Prospective Studies", "Young Adult"], "FINAL_DECISION": "no"}
{"idx": "14631523", "QUESTION": "Sub-classification of low-grade cerebellar astrocytoma: is it clinically meaningful?", "CONCLUSION": "The objectives were to identify prognostic factors for the survival of children with cerebellar astrocytoma, and to evaluate the reproducibility and prognostic value of histological sub-classification and grading.\nChildren aged 0-14 years treated in Denmark for a cerebellar astrocytoma in the period 1960-1984 were included and followed until January 2001 or until their death. The histological specimens from each patient were reviewed for revised grading and classification according to three different classification schemes: the WHO, the Kernohan and the Daumas-Duport grading systems.\nThe overall survival rate was 81% after a follow-up time of 15-40 years. The significant positive prognostic factors for survival were \"surgically gross-total removal\" of the tumour at surgery and location of the tumour in the cerebellum proper as opposed to location in the fourth ventricle. No difference in survival time was demonstrated when we compared pilocytic astrocytoma and fibrillary astrocytoma. Moreover, we found that the Kernohan and the WHO classification systems had no predictive value and that the Daumas-Duport system is unsuitable as a prognostic tool for low-grade posterior fossa astrocytomas.\nDiscordant observations due to interobserver variability make histological sub-classification of low-grade cerebellar astrocytomas in children insufficient for predicting prognosis and biological behaviour. Similar survival rates in a population of paediatric low-grade cerebellar astrocytomas of grades I and II indicate that tumour grade has no prognostic significance within this group of patients. \"Surgically gross-total removal\", especially if the tumour is located in the fourth ventricle is of the highest importance for long-term survival. Histological sub-classification of the tumours has no predictive value.\n", "TYPE": ["Adolescent", "Astrocytoma", "Cerebellar Neoplasms", "Child", "Child, Preschool", "Female", "Follow-Up Studies", "Histological Techniques", "Humans", "Infant", "Infant, Newborn", "Male", "Neurologic Examination", "Predictive Value of Tests", "Prognosis", "Retrospective Studies", "Survival Rate", "Time Factors", "World Health Organization", "alpha-Crystallin B Chain"], "FINAL_DECISION": "no"}
{"idx": "22668852", "QUESTION": "Do African American women require fewer calories to maintain weight?", "CONCLUSION": "The high prevalence of obesity in African American (AA) women may result, in part, from a lower resting metabolic rate (RMR) than non-AA women. If true, AA women should require fewer calories than non-AA women to maintain weight. Our objective was to determine in the setting of a controlled feeding study, if AA women required fewer calories than non-AA women to maintain weight.\nThis analysis includes 206 women (73% AA), aged 22-75 years, who participated in the Dietary Approaches to Stop Hypertension (DASH) trial-a multicenter, randomized, controlled, feeding study comparing the effects of 3 dietary patterns on blood pressure in individuals with prehypertension or stage 1 hypertension. After a 3-week run-in, participants were randomized to 1 of 3 dietary patterns for 8 weeks. Calorie intake was adjusted during feeding to maintain stable weight. The primary outcome of this analysis was average daily calorie (kcal) intake during feeding.\nAA women had higher baseline weight and body mass index than non-AA women (78.4 vs 72.4 kg, P<.01; 29.0 vs 27.6 kg/m(2), P<.05, respectively). During intervention feeding, mean (SD) kcal was 2168 (293) in AA women and 2073 (284) in non-AA women. Mean intake was 94.7 kcal higher in AA women than in non-AA women (P<.05). After adjustment for potential confounders, there was no difference in caloric intake between AA and non-AA women (\u0394 = -2.8 kcal, P = .95).\nThese results do not support the view that AA women are at greater risk for obesity because they require fewer calories to maintain weight.\n", "TYPE": ["Adult", "African Americans", "Aged", "Basal Metabolism", "Blood Pressure", "Body Composition", "Body Mass Index", "Body Weight", "Diet", "Energy Intake", "Female", "Humans", "Hypertension", "Middle Aged", "Obesity", "Young Adult"], "FINAL_DECISION": "no"}
{"idx": "18019905", "QUESTION": "The use of audit to identify maternal mortality in different settings: is it just a difference between the rich and the poor?", "CONCLUSION": "To illustrate how maternal mortality audit identifies different causes of and contributing factors to maternal deaths in different settings in low- and high-income countries and how this can lead to local solutions in reducing maternal deaths.\nDescriptive study of maternal mortality from different settings and review of data on the history of reducing maternal mortality in what are now high-income countries.\nKalabo district in Zambia, Farafenni division in The Gambia, Onandjokwe district in Namibia, and the Netherlands.\nPopulation of rural areas in Zambia and The Gambia, peri-urban population in Namibia and nationwide data from The Netherlands.\nData from facility-based maternal mortality audits from three African hospitals and data from the latest confidential enquiry in The Netherlands.\nMaternal mortality ratio (MMR), causes (direct and indirect) and characteristics.\nMMR ranged from 10 per 100,000 (the Netherlands) to 1540 per 100,000 (The Gambia). Differences in causes of deaths were characterized by HIV/AIDS in Namibia, sepsis and HIV/AIDS in Zambia, (pre-)eclampsia in the Netherlands and obstructed labour in The Gambia.\nDifferences in maternal mortality are more than just differences between the rich and poor. Acknowledging the magnitude of maternal mortality and harnessing a strong political will to tackle the issues are important factors. However, there is no single, general solution to reduce maternal mortality, and identification of problems needs to be promoted through audit, both national and local.\n", "TYPE": ["Africa", "Cause of Death", "Female", "Hospital Mortality", "Humans", "Maternal Mortality", "Medical Audit", "Netherlands"], "FINAL_DECISION": "no"}
{"idx": "7547656", "QUESTION": "Does continuous intravenous infusion of low-concentration epinephrine impair uterine blood flow in pregnant ewes?", "CONCLUSION": "Bolus intravenous injection of epinephrine can decrease uterine blood flow. This study examined the effects of intravenous infusion of epinephrine on uterine blood flow in the gravid ewe.\nMaternal and fetal vascular catheters and a maternal electromagnetic uterine artery flow probe were implanted in 10 near-term gravid ewes. After recovery, saline, 0.125% bupivacaine, 0.125% bupivacaine with 1:200,000 epinephrine, 0.125% bupivacaine with 1:400,000 epinephrine, and 0.125% bupivacaine with 1:800,000 epinephrine were infused into the maternal superior vena cava. Drugs were infused at 10 mL/h for 30 minutes and then at 20 mL/h for an additional 30 minutes. Animals also received an intravenous bolus of epinephrine 15 micrograms. Throughout all infusions, maternal heart rate, systemic and pulmonary blood pressures, uterine blood flow, cardiac output, and acid-base balance were measured, as well as fetal heart rate, blood pressure, and acid-base balance.\nEpinephrine 15 micrograms decreased uterine blood flow to 68 +/- 14% of baseline (mean +/- SD). Infusion of all solutions had no effect on any measured hemodynamic variable.\nIn gravid ewes, intravenous infusion of<or = 1.67 micrograms/min epinephrine altered neither maternal hemodynamics nor uterine blood flow. To the extent that sheep data can be extrapolated to humans, these results suggest that continuous intravenous infusion of epinephrine in local anesthetic solutions is safe if the epidural catheter should enter a blood vessel during the infusion.\n", "TYPE": ["Anesthesia, Obstetrical", "Animals", "Dose-Response Relationship, Drug", "Epinephrine", "Female", "Fetus", "Hemodynamics", "Infusions, Intravenous", "Pregnancy", "Pregnancy, Animal", "Sheep", "Uterus"], "FINAL_DECISION": "no"}
{"idx": "22522271", "QUESTION": "Is late-night salivary cortisol a better screening test for possible cortisol excess than standard screening tests in obese patients with Type 2 diabetes?", "CONCLUSION": "Forty obese patients with T2DM without clinical features of Cushing's syndrome were recruited. Plasma, urinary and salivary cortisol were measured directly by an enzyme-linked immunosorbent assay using monoclonal antibodies. The specificities of the three tests using various cutoffs were calculated and compared, employing the assumption that none of the patients had hypercortisolism.\nThe patients had a mean age and BMI of 56 years (range 31-75) and 37 kg/m\u00b2 (31-56) respectively. All 40 provided late-night salivary cortisol samples. Thirty-eight patients completed all three tests. Two patients only completed two screening tests. The specificities of late-night salivary cortisol (cutoff 10 nmol/L), 24hr UFC (400 nmol) and 1mg DST (50 nmol/L) were 70% (95% CI 53-83%), 90% (76-97%) and 72% (55-85%) respectively. The specificity of late-night salivary cortisol was significantly less than 24 hr UFC (P=0.039) but not 1mg DST (P>0.99).\nLate-night salivary cortisol has a poor specificity for cortisol excess in obese patients with T2DM with 24 hr UFC showing significantly better specificity in our population.\n", "TYPE": ["Adult", "Aged", "Cushing Syndrome", "Diabetes Mellitus, Type 2", "Female", "Humans", "Hydrocortisone", "Male", "Middle Aged", "Obesity", "Saliva", "Time Factors", "Urinalysis"], "FINAL_DECISION": "no"}
{"idx": "21889895", "QUESTION": "Will CT ordering practices change if we educate residents about the potential effects of radiation exposure?", "CONCLUSION": "The aim of this study was to determine if educating residents about the potential effects of radiation exposure from computed tomographic (CT) imaging alters ordering patterns. This study also explored whether referring physicians are interested in radiation education and was an initial effort to address their CT ordering behavior.\nTwo to four months after a radiologist's lecture on the potential effects of radiation exposure related to CT scans, urology and orthopedic residents were surveyed regarding the number and types of CT scans they ordered, the use of alternative imaging modalities, and whether they used the lecture information to educate patients.\nTwenty-one resident lecture attendants completed the survey. The number of CT scans ordered after the lecture stayed constant for 90% (19 of 21) and decreased for 10% (two of 21). The types of CT scans ordered changed after the lecture for 14% (three of 21). Thirty-three percent (seven of 21) reported increases in alternative imaging after the lecture, including 24% (five of 21) reporting increases in magnetic resonance imaging and 19% (four of 21) reporting increases in ultrasound. Patients directed questions about radiation exposure to 57% (12 of 21); 38% (eight of 21) used the lecture information to educate patients. Referring physicians were interested in the topic, and afterward, other physician groups requested radiation education lectures.\nMost clinicians did not change their CT scan ordering after receiving education about radiation from a radiologist. Radiation education allowed clinicians to discuss CT benefits and risks with their patients and to choose appropriate CT protocols. Referring physician groups are interested in this topic, and radiologists should be encouraged to give radiation lectures to them.\n", "TYPE": ["Academic Medical Centers", "Education, Medical, Graduate", "Humans", "Internship and Residency", "Patient Education as Topic", "Practice Patterns, Physicians'", "Radiation Dosage", "Radiation Protection", "Radiology", "Risk", "Tomography, X-Ray Computed"], "FINAL_DECISION": "no"}
{"idx": "16418930", "QUESTION": "Landolt C and snellen e acuity: differences in strabismus amblyopia?", "CONCLUSION": "Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\n100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\nDifferences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\nUsing the charts described, there was only a slight overestimation of visual acuity by the Snellen E compared to the Landolt C, even in strabismus amblyopia. Small differences in the lower visual acuity range have to be considered.\n", "TYPE": ["Adolescent", "Adult", "Aged", "Aged, 80 and over", "Amblyopia", "Cataract", "Child", "Eye Diseases", "Female", "Humans", "Male", "Middle Aged", "Reference Values", "Refractive Errors", "Reproducibility of Results", "Retinal Diseases", "Strabismus", "Vision Tests", "Visual Acuity"], "FINAL_DECISION": "no"}
{"idx": "10798511", "QUESTION": "Blunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?", "CONCLUSION": "Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations.\nWe reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS) score of 15, and unremarkable abdominal examination.\nOf 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.\nThe incidence of abdominal injury in intoxicated, hemodynamically stable, blunt trauma patients with a normal abdominal examination and normal mentation is low. Physical examination and attention to clinical risk factors allow accurate abdominal evaluation without CT.\n", "TYPE": ["Abdominal Injuries", "Adolescent", "Adult", "Aged", "Aged, 80 and over", "Alcoholic Intoxication", "Female", "Humans", "Male", "Middle Aged", "Physical Examination", "Radiography", "Risk Factors", "Trauma Centers"], "FINAL_DECISION": "no"}
{"idx": "25614468", "QUESTION": "Preoperative locoregional staging of gastric cancer: is there a place for magnetic resonance imaging?", "CONCLUSION": "The aim of this study was to prospectively compare the diagnostic performance of magnetic resonance imaging (MRI), multidetector computed tomography (MDCT) and endoscopic ultrasonography (EUS) in the preoperative locoregional staging of gastric cancer.\nThis study had Institutional Review Board approval, and informed consent was obtained from all patients. Fifty-two patients with biopsy-proven gastric cancer underwent preoperative 1.5-T MRI, 64-channel MDCT and EUS. All images were analysed blind, and the results were compared with histopathological findings according to the seventh edition of the TNM classification. After the population had been divided on the basis of the local invasion (T1-3 vs T4a-b) and nodal involvement (N0 vs N+), sensitivity, specificity, positive and negative predictive value, and accuracy were calculated and diagnostic performance measures were assessed using the McNemar test.\nFor T staging, EUS showed higher sensitivity (94%) than MDCT and MRI (65 and 76%; p = 0.02 and p = 0.08). MDCT and MRI had significantly higher specificity (91 and 89%) than EUS (60%) (p = 0.0009 and p = 0.003). Adding MRI to MDCT or EUS did not result in significant differences for sensitivity. For N staging, EUS showed higher sensitivity (92%) than MRI and MDCT (69 and 73%; p = 0.01 and p = 0.02). MDCT showed better specificity (81%) than EUS and MRI (58 and 73%; p = 0.03 and p = 0.15).\nOur prospective study confirmed the leading role of EUS and MDCT in the staging of gastric cancer and did not prove, at present, the value of the clinical use of MRI.\n", "TYPE": ["Adult", "Aged", "Aged, 80 and over", "Endosonography", "Female", "Humans", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Multidetector Computed Tomography", "Multimodal Imaging", "Neoplasm Staging", "Preoperative Care", "Prospective Studies", "Sensitivity and Specificity", "Stomach Neoplasms"], "FINAL_DECISION": "no"}
{"idx": "22513023", "QUESTION": "Do Indigenous Australians age prematurely?", "CONCLUSION": "To assess whether Indigenous Australians age prematurely compared with other Australians, as implied by Australian Government aged care policy, which uses age 50 years and over for population-based planning for Indigenous people compared with 70 years for non-indigenous people.\nCross-sectional analysis of aged care assessment, hospital and health survey data comparing Indigenous and non-indigenous age-specific prevalence of health conditions. Analysis of life tables for Indigenous and non-indigenous populations comparing life expectancy at different ages.\nAt age 63 for women and age 65 for men, Indigenous people had the same life expectancy as non-indigenous people at age 70. There is no consistent pattern of a 20-year lead in age-specific prevalence of age-associated conditions for Indigenous compared with other Australians. There is high prevalence from middle-age onwards of some conditions, particularly diabetes (type unspecified), but there is little or no lead for others.\nThe idea that Indigenous people age prematurely is not well supported by this study of a series of discrete conditions. The current focus and type of services provided by the aged care sector may not be the best way to respond to the excessive burden of chronic disease and disability of middle-aged Indigenous people.\n", "TYPE": ["Adult", "Aged", "Aged, 80 and over", "Australia", "Cross-Sectional Studies", "Geriatric Assessment", "Geriatric Nursing", "Health Policy", "Health Status Indicators", "Humans", "Life Expectancy", "Life Tables", "Middle Aged", "Oceanic Ancestry Group"], "FINAL_DECISION": "no"}
{"idx": "21946341", "QUESTION": "Is there a relationship between complex fractionated atrial electrograms recorded during atrial fibrillation and sinus rhythm fractionation?", "CONCLUSION": "Ablation of persistent atrial fibrillation (AF) may require adjunctive methods of substrate modification. Both ablation-targeting complex fractionated atrial electrograms (CFAEs) recorded during AF and fractionated electrograms recorded during sinus rhythm (sinus rhythm fractionation [SRF]) have been described. However, the relationship of CFAEs with SRF is unclear.\nTwenty patients (age 62 \u00b1 9 years, 13 males) with persistent AF and 9 control subjects without organic heart disease or AF (age 36 \u00b1 6 years, 4 males) underwent detailed CFAE and SRF left atrial electroanatomic maps. The overlap in left atrial regions with CFAEs and SRF was compared in the AF population, and the distribution of SRF was compared among patients with AF and normal controls. Propagation maps were analyzed to identify the activation patterns associated with SR fractionation.\nSRF (338 \u00b1 150 points) and CFAE (418 \u00b1 135 points) regions comprised 29% \u00b1 14% and 25% \u00b1 15% of the left atrial surface area, respectively. There was no significant correlation between SRF and CFAE maps (r = .2; P = NS). On comparing patients with AF and controls, no significant difference was found in the distribution of SRF between groups (P = .74). Regions of SRF overlapped areas of wave-front collision 75% \u00b1 13% of the time.\n(1) There is little overlap between regions of CFAEs during AF and regions of SRF measured in the time domain or the frequency domain, (2) the majority of SRF appears to occur in regions with wave-front collision, (3) the distribution of SRF is similar in patients with AF and normal controls, suggesting that this may not have an important role in AF maintenance and may not be a suitable ablation target.\n", "TYPE": ["Aged", "Arrhythmias, Cardiac", "Atrial Fibrillation", "Catheter Ablation", "Electrophysiologic Techniques, Cardiac", "Female", "Heart Atria", "Humans", "Male", "Middle Aged", "Signal Processing, Computer-Assisted"], "FINAL_DECISION": "no"}
{"idx": "8566975", "QUESTION": "Serovar specific immunity to Neisseria gonorrhoeae: does it exist?", "CONCLUSION": "To determine whether the host immune response to gonorrhoea provides limited serovar specific protection from reinfection.\n508 episodes of gonorrhoea diagnosed at a city centre genitourinary medicine clinic including 22 patients with multiple infections over a 4 year period.\nPatients with recurrent gonococcal infection were analysed with respect to the initial and subsequent serovars isolated.\nNo significant difference was seen in the prevalence of serovars isolated following a repeat infection compared with those without repeat infections. The site of the initial infection did not appear to influence the subsequent serovar isolated.\nWe found no evidence of serovar specific immunity in our population. It remains possible that populations with a higher prevalence of gonorrhoea and more frequent infections may have a quantitatively greater immune response.\n", "TYPE": ["Adolescent", "Adult", "Antibodies, Bacterial", "Antibody Specificity", "Female", "Gonorrhea", "Humans", "Male", "Neisseria gonorrhoeae", "Recurrence", "Serotyping", "Sexual Behavior"], "FINAL_DECISION": "no"}
{"idx": "24922528", "QUESTION": "The association of puberty and young adolescent alcohol use: do parents have a moderating role?", "CONCLUSION": "To explore the extent to which parent-adolescent emotional closeness, family conflict, and parental permissiveness moderate the association of puberty and alcohol use in adolescents (aged 10-14).\nCross-sectional survey of 7631 adolescents from 231 Australian schools. Measures included pubertal status, recent (30day) alcohol use, parent-adolescent emotional closeness, family conflict, parental permissiveness of alcohol use and peer alcohol use. The analysis was based on a two-level (individuals nested within schools) logistic regression model, with main effects entered first, and interaction terms added second.\nThe interaction of family factors and pubertal stage did not improve the fit of the model, so a main effect model of family factors and pubertal stage was adopted. There were significant main effects for pubertal stage with boys in middle puberty at increased odds of alcohol use, and girls in advanced puberty at increased odds of alcohol use.\nPuberty and family factors were strong predictors of adolescent alcohol use, but family factors did not account for variation in the association of pubertal stage and alcohol use.\n", "TYPE": ["Adolescent", "Alcohol Drinking", "Australia", "Child", "Cross-Sectional Studies", "Family Conflict", "Female", "Humans", "Logistic Models", "Male", "Odds Ratio", "Parent-Child Relations", "Parents", "Puberty"], "FINAL_DECISION": "no"}
{"idx": "9542484", "QUESTION": "Does successful completion of the Perinatal Education Programme result in improved obstetric practice?", "CONCLUSION": "To determine whether successful completion of the Perinatal Education Programme (PEP) improves obstetric practice.\nThe three midwife obstetric units (MOUs) in a health district of Mpumalanga were included in the study. Two MOUs enrolled in the PEP and the third did not. A 'before-and-after' study design was used to assess any changes in practice, and to monitor whether any changes occurred in the district during the time of the study; data were also collected at the third MOU. Data were collected by scoring of the obstetric files after the patient had delivered.\nWe ascertained whether the obstetric history, syphilis testing, blood group testing, haemoglobin measurement and uterine growth assessment were performed during antenatal care along with whether appropriate action was taken. For intrapartum care, estimation of fetal weight, the performance of pelvimetry, blood pressure monitoring, urine testing, evaluation of head above pelvis, fetal heart rate monitoring, monitoring of contractions and plotting of cervical dilatation, and whether the appropriate actions were taken, were assessed.\nEight of the 13 midwives at the two MOUs completed the PEP and all demonstrated an improvement in knowledge. Case notes of 303 patients from the various clinics were studied. There was no change in the referral patterns of any of the clinics during the study period. The obstetric history was well documented, but in no group was there a satisfactory response to a detected problem; appropriate action was taken in between 0% and 12% of cases. Syphilis testing was performed in 56-82% of cases, with no difference between the groups. The haemoglobin level was measured in only 4-15% of patients, with no difference before or after completion of the PEP. Where a problem in uterine growth was detected, an appropriate response occurred in 0-8% of patients and no difference before or after completion of the PEP was ascertained. In all groups, estimation of fetal weight and pelvimetry were seldom performed, the urine and fetal heart rate documentation were moderately well done and the blood pressure monitoring, assessment of head above pelvis, monitoring of contractions and plotting of cervical dilatation were usually performed. No differences before or after the PEP were detected. Where problems were detected, appropriate actions taken during labour improved, but not significantly.\nCompletion of the obstetric manual of the PEP improved the knowledge of the midwives but no alteration in practice was detected.\n", "TYPE": ["Africa", "Female", "Humans", "Midwifery", "Perinatal Care", "Pregnancy", "Prenatal Care", "Rural Population"], "FINAL_DECISION": "no"}
{"idx": "25752912", "QUESTION": "Is the probability of prenatal diagnosis or termination of pregnancy different for fetuses with congenital anomalies conceived following assisted reproductive techniques?", "CONCLUSION": "To compare the probability of prenatal diagnosis (PND) and termination of pregnancy for fetal anomaly (TOPFA) between fetuses conceived by assisted reproductive techniques (ART) and spontaneously-conceived fetuses with congenital heart defects (CHD).\nPopulation-based observational study.\nParis and surrounding suburbs.\nFetuses with CHD in the Paris registry of congenital malformations and cohort of children with CHD (Epicard).\nComparison of ART-conceived and spontaneously conceived fetuses taking into account potential confounders (maternal characteristics, multiplicity and year of birth or TOPFA).\nProbability and gestational age at PND and TOPFA for ART-conceived versus spontaneously conceived fetuses.\nThe probability of PND (28.1% versus 34.6%, P = 0.077) and TOPFA (36.2% versus 39.2%, P = 0.677) were not statistically different between ART-conceived (n = 171) and spontaneously conceived (n = 4620) fetuses. Estimates were similar after adjustment for potential confounders. Gestational age at PND tended to be earlier for ART fetuses (23.1 versus 24.8 weeks, P = 0.05) but no statistical difference was found after adjustment for confounders. Gestational age at TOPFA was comparable between ART-conceived and spontaneously conceived fetuses.\nIn our population, ART conception was not significantly associated with the probability of PND or TOPFA for CHD. One implication of our results is that live births may be adequate for assessing the overall risk of CHD related to ART. However, total prevalence, in particular of severe CHD, would not be adequately assessed if TOPFA are not included.\n", "TYPE": ["Abortion, Induced", "Adult", "Female", "Fetal Diseases", "Gestational Age", "Heart Defects, Congenital", "Humans", "Pregnancy", "Prenatal Diagnosis", "Reproductive Techniques, Assisted", "Risk Factors", "Young Adult"], "FINAL_DECISION": "no"}
{"idx": "11862129", "QUESTION": "Do clinical variables predict pathologic radiographs in the first episode of wheezing?", "CONCLUSION": "To determine if clinical variables assessed in relation to Albuterol aerosol treatments accurately identify children with pathologic radiographs during their initial episode of bronchospasm.\nA prospective convenience sample of children with a first episode of wheezing. Data collected included demographics, baseline and post-treatment clinical score and physical examination, number of aerosols, requirement for supplemental oxygen, and disposition. Chest radiographs were obtained and interpreted, and patients were divided into 2 groups based on a pathologic versus nonpathologic radiograph interpretation. Chi2 testing was performed for categoric variables, and the student t test was performed for continuous variables. A discriminant analysis was used to develop a model.\nPathologic radiographs were identified in 61 patients (9%). Between groups, a significant difference was noted for pretreatment oxygen saturation only. Clinical score, respiratory rate, and presence of rales both pretreatment and posttreatment were not significantly different between groups. The discriminant analysis correctly predicted 90% of nonpathologic radiographs but only 15% of pathologic radiographs.\nClinical variables, either isolated or as components of a model, could not identify all children with pathologic radiographs.\n", "TYPE": ["Adolescent", "Albuterol", "Bronchodilator Agents", "Child", "Child, Preschool", "Discriminant Analysis", "Female", "Humans", "Infant", "Male", "Multivariate Analysis", "Physical Examination", "Prospective Studies", "Radiography", "Respiratory Sounds", "Respiratory Tract Diseases", "Sensitivity and Specificity", "Statistics, Nonparametric"], "FINAL_DECISION": "no"}
{"idx": "22236315", "QUESTION": "Is distance to provider a barrier to care for medicaid patients with breast, colorectal, or lung cancer?", "CONCLUSION": "Distance to provider might be an important barrier to timely diagnosis and treatment for cancer patients who qualify for Medicaid coverage. Whether driving time or driving distance is a better indicator of travel burden is also of interest.\nDriving distances and times from patient residence to primary care provider were calculated for 3,917 breast, colorectal (CRC) and lung cancer Medicaid patients in Washington State from 1997 to 2003 using MapQuest.com. We fitted regression models of stage at diagnosis and time-to-treatment (number of days between diagnosis and surgery) to test the hypothesis that travel burden is associated with timely diagnosis and treatment of cancer.\nLater stage at diagnosis for breast cancer Medicaid patients is associated with travel burden (OR = 1.488 per 100 driving miles, P= .037 and OR = 1.270 per driving hour, P= .016). Time-to-treatment after diagnosis of CRC is also associated with travel burden (14.57 days per 100 driving miles, P= .002 and 5.86 days per driving hour, P= .018).\nAlthough travel burden is associated with timely diagnosis and treatment for some types of cancer, we did not find evidence that driving time was, in general, better at predicting timeliness of cancer diagnosis and treatment than driving distance. More intensive efforts at early detection of breast cancer and early treatment of CRC for Medicaid patients who live in remote areas may be needed.\n", "TYPE": ["Adolescent", "Adult", "Breast Neoplasms", "Colorectal Neoplasms", "Female", "Health Personnel", "Health Services Accessibility", "Humans", "Lung Neoplasms", "Male", "Medicaid", "Middle Aged", "Neoplasm Staging", "United States", "Washington", "Young Adult"], "FINAL_DECISION": "no"}
{"idx": "21361755", "QUESTION": "Laminoplasty outcomes: is there a difference between patients with degenerative stenosis and those with ossification of the posterior longitudinal ligament?", "CONCLUSION": "Two common causes of cervical myelopathy include degenerative stenosis and ossification of the posterior longitudinal ligament (OPLL). It has been postulated that patients with OPLL have more complications and worse outcomes than those with degenerative stenosis. The authors sought to compare the surgical results of laminoplasty in the treatment of cervical stenosis with myelopathy due to either degenerative changes or segmental OPLL.\nThe authors conducted a retrospective review of 40 instrumented laminoplasty cases performed at a single institution over a 4-year period to treat cervical myelopathy without kyphosis. Twelve of these patients had degenerative cervical stenotic myelopathy ([CSM]; degenerative group), and the remaining 28 had segmental OPLL (OPLL group). The 2 groups had statistically similar demographic characteristics and number of treated levels (mean 3.9 surgically treated levels; p>0.05). The authors collected perioperative and follow-up data, including radiographic results.\nThe overall clinical follow-up rate was 88%, and the mean clinical follow-up duration was 16.4 months. The mean radiographic follow-up rate was 83%, and the mean length of radiographic follow-up was 9.3 months. There were no significant differences in the estimated blood loss (EBL) or length of hospital stay (LOS) between the groups (p>0.05). The mean EBL and LOS for the degenerative group were 206 ml and 3.7 days, respectively. The mean EBL and LOS for the OPLL group were 155 ml and 4 days, respectively. There was a statistically significant improvement of more than one grade in the Nurick score for both groups following surgery (p<0.05). The Nurick score improvement was not statistically different between the groups (p>0.05). The visual analog scale (VAS) neck pain scores were similar between groups pre- and postoperatively (p>0.05). The complication rates were not statistically different between groups either (p>0.05). Radiographically, both groups lost extension range of motion (ROM) following laminoplasty, but this change was not statistically significant (p>0.05).\nPatients with CSM due to either degenerative disease or segmental OPLL have similar perioperative results and neurological outcomes with laminoplasty. The VAS neck pain scores did not improve significantly with laminoplasty for either group. Laminoplasty may limit extension ROM.\n", "TYPE": ["Cervical Vertebrae", "Constriction, Pathologic", "Female", "Follow-Up Studies", "Humans", "Laminectomy", "Longitudinal Ligaments", "Male", "Middle Aged", "Neurodegenerative Diseases", "Ossification of Posterior Longitudinal Ligament", "Radiography", "Retrospective Studies", "Spinal Cord Diseases", "Treatment Outcome"], "FINAL_DECISION": "no"}
{"idx": "11438275", "QUESTION": "Does patient position during liver surgery influence the risk of venous air embolism?", "CONCLUSION": "It is generally believed that positioning of the patient in a head-down tilt (Trendelenberg position) decreases the likelihood of a venous air embolism during liver resection.\nThe physiological effect of variation in horizontal attitude on central and hepatic venous pressure was measured in 10 patients during liver surgery. Hemodynamic indices were recorded with the operating table in the horizontal, 20 degrees head-up and 20 degrees head-down positions.\nThere was no demonstrable pressure gradient between the hepatic and central venous levels in any of the positions. The absolute pressures did, however, vary in a predictable way, being highest in the head-down and lowest during head-up tilt. However, on no occasion was a negative intraluminal pressure recorded.\nThe effect on venous pressures caused by the change in patient positioning alone during liver surgery does not affect the risk of venous air embolism.\n", "TYPE": ["Adult", "Aged", "Central Venous Pressure", "Embolism, Air", "Female", "Head-Down Tilt", "Hepatectomy", "Hepatic Veins", "Humans", "Male", "Middle Aged", "Posture", "Risk Factors", "Vena Cava, Inferior", "Venous Pressure"], "FINAL_DECISION": "no"}
{"idx": "16778275", "QUESTION": "Is routine chest radiography after transbronchial biopsy necessary?", "CONCLUSION": "Pneumothorax following flexible bronchoscopy (FB) with transbronchial biopsy (TBB) occurs in 1 to 6% of cases. Routine chest radiography (CXR) following TBB is therefore requested by most pulmonologists in an attempt to detect complications, particularly pneumothorax. The objective of this study was to determine if routine CXR after bronchoscopy and TBB is necessary.\nThe study group included 350 consecutive patients who underwent FB with TBB at our institution between December 2001 and January 2004. Routine CXR was performed up to 2 h after the procedure in all cases. Additionally, the following information was recorded in all patients: sex, age, immune status, indication for bronchoscopy, total number of biopsies done, segment sampled, pulse oxygen saturation, and development of symptoms suggestive of pneumothorax.\nPneumothorax was diagnosed radiologically in 10 patients (2.9%). Seven patients had symptoms strongly suggestive of pneumothorax prior to CXR, including four patients with large (>10%) pneumothorax. The other three patients were asymptomatic, with only minimal pneumothorax (</= 10%), which resolved completely 24 to 48 h later.\nWe conclude that routine CXR after bronchoscopy with TBB is necessary only in patients with symptoms suggestive of pneumothorax. In asymptomatic patients, pneumothorax is rare and usually small, so routine CXR is not necessary in this category of patients.\n", "TYPE": ["Adult", "Aged", "Biopsy", "Bronchi", "Bronchoscopy", "Diagnostic Tests, Routine", "Female", "Follow-Up Studies", "Humans", "Lung Diseases", "Male", "Middle Aged", "Needs Assessment", "Pneumothorax", "Prospective Studies", "Radiography, Thoracic"], "FINAL_DECISION": "no"}
{"idx": "17051586", "QUESTION": "Can folic acid protect against congenital heart defects in Down syndrome?", "CONCLUSION": "Several studies have suggested a protective effect of folic acid (FA) on congenital heart anomalies. Down syndrome (DS) infants are known to have a high frequency of heart anomalies. Not all children with DS suffer from heart anomalies, which raises the question whether maternal factors might affect the risk of these anomalies. Our objectives were to investigate whether first-trimester FA use protects against heart anomalies among DS children.\nWomen with liveborn DS children participating in the Slone Epidemiology Center Birth Defects Study between 1976 and 1997 were included. We performed case-control analyses using DS, with heart anomalies as cases and DS, without heart anomalies as controls. Subanalyses were performed for defects that have been associated with FA in non-DS populations (conotruncal, ventricular septal [VSD]) and for those that are associated with DS (ostium secundum type atrial septal defects [ASD]and endocardial cushion defects [ECD]). Exposure was defined as the use of any FA-containing product for an average of at least 4 days per week during the first 12 weeks of pregnancy, whereas no exposure was defined as no use of FA in these 12 weeks.\nOf the 223 cases, 110 (49%) were exposed versus 84 (46%) of the 184 controls. After adjustment for possible confounders, no protective effect of FA was found on heart anomalies overall (OR 0.95, 95% CI: 0.61-1.47) nor separately for conotruncal defects, VSDs, ASDs, or ECDs.\nOur study does not show a protective effect of FA on heart anomalies among infants with DS.\n", "TYPE": ["Dietary Supplements", "Down Syndrome", "Female", "Folic Acid", "Food, Fortified", "Heart Defects, Congenital", "Humans", "Infant, Newborn", "Male", "Pregnancy", "Pregnancy Trimester, First", "Retrospective Studies"], "FINAL_DECISION": "no"}
{"idx": "23497210", "QUESTION": "Are women with major depression in pregnancy identifiable in population health data?", "CONCLUSION": "Although record linkage of routinely collected health datasets is a valuable research resource, most datasets are established for administrative purposes and not for health outcomes research. In order for meaningful results to be extrapolated to specific populations, the limitations of the data and linkage methodology need to be investigated and clarified. It is the objective of this study to investigate the differences in ascertainment which may arise between a hospital admission dataset and a dispensing claims dataset, using major depression in pregnancy as an example. The safe use of antidepressants in pregnancy is an ongoing issue for clinicians with around 10% of pregnant women suffer from depression. As the birth admission will be the first admission to hospital during their pregnancy for most women, their use of antidepressants, or their depressive condition, may not be revealed to the attending hospital clinicians. This may result in adverse outcomes for the mother and infant.\nPopulation-based de-identified data were provided from the Western Australian Data Linkage System linking the administrative health records of women with a delivery to related records from the Midwives' Notification System, the Hospital Morbidity Data System and the national Pharmaceutical Benefits Scheme dataset. The women with depression during their pregnancy were ascertained in two ways: women with dispensing records relating to dispensed antidepressant medicines with an WHO ATC code to the 3rd level, pharmacological subgroup, 'N06A Antidepressants'; and, women with any hospital admission during pregnancy, including the birth admission, if a comorbidity was recorded relating to depression.\nFrom 2002 to 2005, there were 96698 births in WA. At least one antidepressant was dispensed to 4485 (4.6%) pregnant women. There were 3010 (3.1%) women with a comorbidity related to depression recorded on their delivery admission, or other admission to hospital during pregnancy. There were a total of 7495 pregnancies identified by either set of records. Using data linkage, we determined that these records represented 6596 individual pregnancies. Only 899 pregnancies were found in both groups (13.6% of all cases). 80% of women dispensed an antidepressant did not have depression recorded as a comorbidity on their hospital records. A simple capture-recapture calculation suggests the prevalence of depression in this population of pregnant women to be around 16%.\nNo single data source is likely to provide a complete health profile for an individual. For women with depression in pregnancy and dispensed antidepressants, the hospital admission data do not adequately capture all cases.\n", "TYPE": ["Adult", "Antidepressive Agents", "Australia", "Databases, Factual", "Depressive Disorder, Major", "Female", "Hospital Records", "Humans", "Longitudinal Studies", "Medical Record Linkage", "Medical Records Systems, Computerized", "Pregnancy", "Pregnancy Complications", "Prevalence"], "FINAL_DECISION": "no"}
{"idx": "22382608", "QUESTION": "SPECT study with I-123-Ioflupane (DaTSCAN) in patients with essential tremor. Is there any correlation with Parkinson's disease?", "CONCLUSION": "The differential diagnosis between essential tremor (ET) and Parkinson's disease (PD) may be, in some cases, very difficult on clinical grounds alone. In addition, it is accepted that a small percentage of ET patients presenting symptoms and signs of possible PD may progress finally to a typical pattern of parkinsonism. Ioflupane, N-u-fluoropropyl-2a-carbomethoxy-3a-(4-iodophenyl) nortropane, also called FP-CIT, labelled with (123)I (commercially known as DaTSCAN) has been proven to be useful in the differential diagnosis between PD and ET and to confirm dopaminergic degeneration in patients with parkinsonism. The aim of this study is to identify dopaminergic degeneration in patients with PD and distinguish them from others with ET using semi-quantitative SPECT (123)I-Ioflupane (DaTSCAN) data in comparison with normal volunteers (NV), in addition with the respective ones of patients referred as suffering from ET, as well as, of patients with a PD diagnosis at an initial stage with a unilateral presentation of motor signs.\nTwenty-eight patients suffering from ET (10 males plus 18 females) and 28 NV (12 males and 16 females) were enroled in this study. In addition, 33 patients (11 males and 22 females) with an established diagnosis of PD with unilateral limb involvement (12 left hemi-body and 21 right hemi-body) were included for comparison with ET. We used DaTSCAN to obtain SPECT images and measure the radiopharmaceutical uptake in the striatum (S), as well as the caudate nucleus (CN) and putamen (P) in all individuals.\nQualitative (Visual) interpretation of the SPECT data did not find any difference in the uptake of the radiopharmaceutical at the level of the S, CN and P between NV and ET patients. Reduced accumulation of the radiopharmaceutical uptake was found in the P of all PD patients. Semiquantitative analysis revealed significant differences between NV and ET patients in the striatum, reduced in the latter. There was also a significant reduction in the tracer accumulation in the left putamen of patients with right hemi-parkinsonism compared to ET and NV. Patients with left hemi-parkinsonism, demonstrated reduced radioligand uptake in the right putamen in comparison with ET and NV. Clinical follow-up of 20 patients with ET at (so many months afterwards) revealed no significant change in clinical presentation, particularly no signs of PD. Follow-up DaTSCAN performed in 10 of them (so many months afterwards) was negative in all but one. This one had an equivocal baseline study which deteriorated 12\u00a0months later.\nOur results do not support the hypothesis of a link between essential tremor and Parkinson's disease. However, it appears that ET patients have a small degree of striatal dopaminergic degeneration. If this is due to alterations in the nigrostriatl pathway or of other origin it is not clear. Follow-up studies of essential tremor patients are warranted to assess progression of disease and to understand better the possible cause for striatal dopaminergic degeneration.\n", "TYPE": ["Case-Control Studies", "Diagnosis, Differential", "Dopamine", "Essential Tremor", "Female", "Humans", "Male", "Motor Activity", "Nortropanes", "Parkinson Disease", "Tomography, Emission-Computed, Single-Photon"], "FINAL_DECISION": "no"}
{"idx": "22876568", "QUESTION": "Is vitamin D deficiency a feature of pediatric celiac disease?", "CONCLUSION": "Celiac disease (CD) is an autoimmune enteropathy characterized by villus atrophy and malabsorption of essential nutrients. Vitamin D deficiency has been described in autoimmune diseases, but its status in prepubertal children with CD has not been adequately studied.\nTo determine the vitamin D status of prepubertal children with CD.\nA retrospective study of prepubertal children aged 3-12 years with CD (n=24) who were compared to prepubertal, non-CD children of the same age (n=50). Children were included in the study if they had a diagnosis of CD by intestinal biopsy, and were not on a gluten-free diet (GFD). Patients were excluded if they had diseases of calcium or vitamin D metabolism, or were receiving calcium or vitamin D supplementation or had other autoimmune diseases. All subjects had their serum 25-hydroxyvitamin D [25(OH)D] level measured.\nThere was no difference in 25(OH)D level between the CD and non-CD children (27.58 +/- 9.91 versus 26.20 +/- 10.45, p = 0.59). However, when the patients were subdivided into obese and non-obese groups, the non-obese CD patients had a significantly higher 25(OH)D level than the obese normal children (28.39 +/- 10.26 versus 21.58 +/- 5.67, p = 0.009). In contrast, there was no difference in 25(OH)D level between non-obese CD patients and non-obese normal children (28.39 +/- 10.26 versus 30.64 +/-12.08, p = 0.52). The season of 25(OH)D measurement was not a significant confounder (p =0.7).\nOur data showed no difference in 25(OH) D levels between normal children and those with CD when adjusted for body mass index.\n", "TYPE": ["Biopsy", "Celiac Disease", "Child", "Child, Preschool", "Female", "Humans", "Intestinal Absorption", "Intestinal Mucosa", "Male", "Retrospective Studies", "Vitamin D", "Vitamin D Deficiency"], "FINAL_DECISION": "no"}
{"idx": "17062234", "QUESTION": "Surgical management of the atherosclerotic ascending aorta: is endoaortic balloon occlusion safe?", "CONCLUSION": "Occlusion of the atherosclerotic ascending aorta by an endoaortic inflatable balloon has been proposed as an alternative to conventional cross-clamping to prevent injury to the vessel and distal embolization of debris. The safety and the effectiveness of endoaortic occlusion have not been documented in this setting.\nEndoaortic occlusion was employed in 52 of 2,172 consecutive patients. Surgeon's choice was based on preoperative identification of aortic calcifications or intraoperative epiaortic ultrasonographic scanning. Deaths and strokes were analyzed casewise and in aggregate.\nIn 10 patients (19.2%), the endoaortic balloon had to be replaced by the ordinary cross-clamp because of incomplete occlusion (n = 5), hindered exposure (n = 2), or balloon rupture (n = 3). In-hospital death occurred in 13 patients (25%), and stroke on awakening from anesthesia in 2 (3.8%). The death rate of patients treated by endoaortic occlusion was significantly higher compared with all other patients (4.2%, p<0.0001) and with the expected estimate by European System for Cardiac Operative Risk Evaluation (10.5%, p = 0.05). By multivariable analysis, use of endoaortic occlusion was independently associated with in-hospital death (odds ratio = 5.609, 95% confidence interval: 2.684 to 11.719). Although the stroke rate was higher in the endoaortic occlusion group compared with all other patients, the difference was only possibly significant (3.8% versus 0.8%, p = 0.067).\nIn this series, the endoaortic occlusion was frequently ineffective, and was associated with a significantly higher risk of in-hospital death and a numerically higher risk of stroke.\n", "TYPE": ["Aged", "Aged, 80 and over", "Aorta", "Aortic Diseases", "Atherosclerosis", "Balloon Occlusion", "Constriction", "Coronary Artery Bypass", "Female", "Hospital Mortality", "Humans", "Male", "Middle Aged", "Retrospective Studies", "Stroke", "Treatment Outcome"], "FINAL_DECISION": "no"}
{"idx": "12070552", "QUESTION": "Do antibiotics decrease post-tonsillectomy morbidity?", "CONCLUSION": "A tonsillectomy audit was carried out and compared with other studies, to emphasize the role of antibiotics.\nThis study was carried out at North West Armed Forces Hospital, Tabuk, Kingdom of Saudi Arabia, during the year January 1999 through to December 1999. This is a retrospective study of patients who had tonsillectomy with or with adenoidectomy, the topics audited included indication for surgery, grade of surgeon, method of surgery, length of hospital stay, complications and the use of postoperative antibiotics.\nA total of 185 patients underwent tonsillectomy with or without adenoidectomy. The patients age ranged between 2 years to 53 years and the majority were children. In our audit we found no difference with regard to grade of surgeons, method of hemostasis in the outcome of surgery. Moreover, postoperative antibiotics had no role in pain control, postoperative fever, secondary hemorrhage or reduction in hospital stay. The administration of analgesics on the basis of, as required, had poor pain control.\nPost tonsillectomy antibiotics did not prove to have a role in minimizing postoperative morbidity. Moreover, analgesics given on the basis of as required had a limited value.\n", "TYPE": ["Adolescent", "Adult", "Anti-Bacterial Agents", "Anti-Inflammatory Agents, Non-Steroidal", "Child", "Child, Preschool", "Female", "Humans", "Male", "Medical Audit", "Middle Aged", "Postoperative Care", "Postoperative Complications", "Retrospective Studies", "Tonsillectomy"], "FINAL_DECISION": "no"}
{"idx": "24739448", "QUESTION": "Have antiepileptic drug prescription claims changed following the FDA suicidality warning?", "CONCLUSION": "In January 2008, the Food and Drug Administration (FDA) communicated concerns and, in May 2009, issued a warning about an increased risk of suicidality for all antiepileptic drugs (AEDs). This research evaluated the association between the FDA suicidality communications and the AED prescription claims among members with epilepsy and/or psychiatric disorder.\nA longitudinal interrupted time-series design was utilized to evaluate Oklahoma Medicaid claims data from January 2006 through December 2009. The study included 9289 continuously eligible members with prevalent diagnoses of epilepsy and/or psychiatric disorder and at least one AED prescription claim. Trends, expressed as monthly changes in the log odds of AED prescription claims, were compared across three time periods: before (January 2006 to January 2008), during (February 2008 to May 2009), and after (June 2009 to December 2009) the FDA warning.\nBefore the FDA warning period, a significant upward trend of AED prescription claims of 0.01% per month (99% CI: 0.008% to 0.013%, p<0.0001) was estimated. In comparison to the prewarning period, no significant change in trend was detected during (-20.0%, 99% CI: -70.0% to 30.0%, p=0.34) or after (80.0%, 99% CI: -20.0% to 200.0%, p=0.03) the FDA warning period. After stratification, no diagnostic group (i.e., epilepsy alone, epilepsy and comorbid psychiatric disorder, and psychiatric disorder alone) experienced a significant change in trend during the entire study period (p>0.01).\nDuring the time period considered, the FDA AED-related suicidality warning does not appear to have significantly affected prescription claims of AED medications for the study population.\n", "TYPE": ["Adolescent", "Adult", "Anticonvulsants", "Child", "Child, Preschool", "Drug Prescriptions", "Epilepsy", "Humans", "Infant", "Interrupted Time Series Analysis", "Medicaid", "Middle Aged", "Suicide", "United States", "United States Food and Drug Administration", "Young Adult"], "FINAL_DECISION": "no"}
{"idx": "24625433", "QUESTION": "Are high flow nasal cannulae noisier than bubble CPAP for preterm infants?", "CONCLUSION": "Noise exposure in the neonatal intensive care unit is believed to be a risk factor for hearing loss in preterm neonates. Continuous positive airway pressure (CPAP) devices exceed recommended noise levels. High flow nasal cannulae (HFNC) are an increasingly popular alternative to CPAP for treating preterm infants, but there are no in vivo studies assessing noise production by HFNC.\nTo study whether HFNC are noisier than bubble CPAP (BCPAP) for preterm infants.\nAn observational study of preterm infants receiving HFNC or BCPAP. Noise levels within the external auditory meatus (EAM) were measured using a microphone probe tube connected to a calibrated digital dosimeter. Noise was measured across a range of frequencies and reported as decibels A-weighted (dBA).\nA total of 21 HFNC and 13 BCPAP noise measurements were performed in 21 infants. HFNC gas flows were 2-5 L/min, and BCPAP gas flows were 6-10 L/min with set pressures of 5-7 cm of water. There was no evidence of a difference in average noise levels measured at the EAM: mean difference (95% CI) of -1.6 (-4.0 to 0.9) dBA for HFNC compared to BCPAP. At low frequency (500 Hz), HFNC was mean (95% CI) 3.0 (0.3 to 5.7) dBA quieter than BCPAP. Noise increased with increasing BCPAP gas flow (p=0.007), but not with increasing set pressure. There was a trend to noise increasing with increasing HFNC gas flows.\nAt the gas flows studied, HFNC are not noisier than BCPAP for preterm infants.\n", "TYPE": ["Catheters", "Continuous Positive Airway Pressure", "Environmental Monitoring", "Humans", "Infant, Newborn", "Infant, Premature", "Infant, Premature, Diseases", "Intensive Care Units, Neonatal", "Nasal Cavity", "Noise", "Noninvasive Ventilation", "Terminology as Topic"], "FINAL_DECISION": "no"}
{"idx": "7497757", "QUESTION": "Cardiopulmonary bypass temperature does not affect postoperative euthyroid sick syndrome?", "CONCLUSION": "To determine if temperature during cardiopulmonary bypass (CPB) has an effect on perioperative and postoperative thyroid function.\nProspective study comparing thyroid function during and after hypothermic and normothermic CPB.\nCardiac surgical unit at a university-affiliated hospital.\nTwelve patients scheduled to undergo cardiac operations with normothermic (n = 6) or hypothermic (n = 6) CPB.\nBlood was analyzed for serum concentration of total thyroxine (TT4), total triiodothyronine (TT3), free T3 (fT3), reverse T3 (rT3), and thyroid stimulating hormone (TSH) preoperatively, 60 min after CPB was initiated, 30 min after discontinuing CPB, and on postoperative days (POD) 1, 3, and 5.\nPatients who underwent either cold (26 degrees +/- 5 degrees C) or warm (35 degrees +/- 1 degree C) CPB were comparable with regard to age, body weight, duration of CPB, cross-clamp time, use of inotropes, total heparin dose, and length of hospital stay. Incidence of postoperative myocardial infarction, congestive heart failure, and death were similar. In both groups, TT4 and TT3 were reduced below baseline values beginning with CPB and persisting for up to 5 days after CPB (p<0.05), free T3 was reduced for up to 3 days after CPB (p<0.05), mean serum rT3 was elevated on POD 1 and POD 3 (p<0.05), and TSH remained unchanged.\nThe results of this study suggest that normothermic CPB does not prevent the development of the \"euthyroid sick syndrome\" during and after CPB. Despite these changes in thyroid function, most patients in both groups had a normal postoperative recovery.\n", "TYPE": ["Body Temperature", "Cardiac Surgical Procedures", "Cardiopulmonary Bypass", "Euthyroid Sick Syndromes", "Humans", "Middle Aged", "Postoperative Complications", "Thyrotropin", "Thyroxine", "Triiodothyronine", "Triiodothyronine, Reverse"], "FINAL_DECISION": "no"}
{"idx": "10456814", "QUESTION": "Does desflurane alter left ventricular function when used to control surgical stimulation during aortic surgery?", "CONCLUSION": "Although desflurane is commonly used to control surgically induced hypertension, its effects on left ventricular (LV) function have not been investigated in this clinical situation. The purpose of the present study was to evaluate the LV function response to desflurane, when used to control intraoperative hypertension.\nIn 50 patients, scheduled for vascular surgery, anesthesia was induced with sufentanil 0.5 microg/kg, midazolam 0.3 mg/kg and atracurium 0.5 mg/kg. After tracheal intubation, anesthesia was maintained with increments of drugs with controlled ventilation (N2O/O2=60/40%) until the start of surgery. A 5 Mhz transesophageal echocardiography (TEE) probe was inserted after intubation. Pulmonary artery catheter and TEE measurements were obtained after induction (to)(control value), at surgical incision (t1) if it was associated with an increase in systolic arterial pressure (SAP) greater than 140 mmHg (hypertension) and after control of hemodynamic parameters by administration of desflurane (return of systolic arterial pressure to within 20% of the control value) (t2) in a fresh gas flow of 31/ min.\nSixteen patients developed hypertension at surgical incision. SAP was controlled by desflurane in all 16 patients. Afterload assessed by systemic vascular resistance index (SVRI), end-systolic wall-stress (ESWS) and left-ventricular stroke work index (LVSWI) increased with incision until the hypertension returned to post-induction values with mean end-tidal concentration of 5.1+/-0.7% desflurane. No change in heart rate, cardiac index, mean pulmonary arterial pressure, stroke volume, end-diastolic and end-systolic cross-sectional areas, fractional area change and left ventricular circumferential fiber shortening was noted when desflurane was added to restore blood pressure.\nThis study demonstrates that in patients at risk for cardiac morbidity undergoing vascular surgery, desflurane is effective to control intraoperative hypertension without fear of major cardiac depressant effect.\n", "TYPE": ["Anesthetics, Inhalation", "Anesthetics, Intravenous", "Aorta", "Atracurium", "Blood Pressure", "Cardiac Output", "Catheterization, Swan-Ganz", "Diastole", "Echocardiography, Transesophageal", "Female", "Heart Rate", "Heart Ventricles", "Humans", "Hypertension", "Intraoperative Complications", "Intubation, Intratracheal", "Isoflurane", "Male", "Midazolam", "Middle Aged", "Neuromuscular Nondepolarizing Agents", "Nitrous Oxide", "Oxygen", "Stroke Volume", "Sufentanil", "Systole", "Vascular Resistance", "Ventricular Function, Left"], "FINAL_DECISION": "no"}
{"idx": "12769830", "QUESTION": "Should tumor depth be included in prognostication of soft tissue sarcoma?", "CONCLUSION": "Most staging systems for soft tissue sarcoma are based on histologic malignancy-grade, tumor size and tumor depth. These factors are generally dichotomized, size at 5 cm. We believe it is unlikely that tumor depth per se should influence a tumor's metastatic capability. Therefore we hypothesized that the unfavourable prognostic importance of depth could be explained by the close association between size and depth, deep-seated tumors on average being larger than the superficial ones. When tumor size is dichotomized, this effect should be most pronounced in the large size (>5 cm) group in which the size span is larger.\nWe analyzed the associations between tumor size and depth and the prognostic importance of grade, size and depth in a population-based series of 490 adult patients with soft tissue sarcoma of the extremity or trunk wall with complete, 4.5 years minimum, follow-up.\nMultivariate analysis showed no major prognostic effect of tumor depth when grade and size were taken into account. The mean size of small tumors was the same whether superficial or deep but the mean size of large and deep-seated tumors were one third larger than that of large but superficial tumors. Tumor depth influenced the prognosis in the subset of high-grade and large tumors. In this subset deep-seated tumors had poorer survival rate than superficial tumors, which could be explained by the larger mean size of the deep-seated tumors.\nMost of the prognostic value of tumor depth in soft tissue sarcomas of the extremity or trunk wall can be explained by the association between tumor size and depth.\n", "TYPE": ["Adolescent", "Adult", "Aged", "Aged, 80 and over", "Female", "Humans", "Male", "Middle Aged", "Multivariate Analysis", "Neoplasm Staging", "Prognosis", "Sarcoma"], "FINAL_DECISION": "no"}
{"idx": "22251324", "QUESTION": "Does performance in selection processes predict performance as a dental student?", "CONCLUSION": "This study investigated associations between the performance of dental students in each of the three components of the selection procedure [academic average, Undergraduate Medicine and Health Sciences Admission Test (UMAT) and structured interview], socio-demographic characteristics and their academic success in an undergraduate dental surgery programme.\nLongitudinal review of admissions data relating to students entering dental education at the University of Otago, New Zealand, between 2004 and 2009 was compared with academic performance throughout the dental programme.\nAfter controlling for variables, pre-admission academic average, UMAT scores and interview performance did not predict performance as a dental student. Class place in second year, however, was a strong predictor of class place in final year. Multivariate analysis demonstrated that the best predictors of higher class placement in the final year were New Zealand European ethnicity and domestic (rather than international) student status. Other socio-demographic characteristics were not associated with performance. These interim findings provide a sound base for the ongoing study.\nThe study found important socio-demographic differences in pre-admission test scores, but those scores did not predict performance in the dental programme, whether measured in second year or in final year.\n", "TYPE": ["Achievement", "Adult", "Chi-Square Distribution", "Clinical Competence", "College Admission Test", "Educational Measurement", "Female", "Humans", "Interviews as Topic", "Longitudinal Studies", "Male", "New Zealand", "Predictive Value of Tests", "School Admission Criteria", "Students, Dental"], "FINAL_DECISION": "no"}
{"idx": "11411430", "QUESTION": "Antral follicle assessment as a tool for predicting outcome in IVF--is it a better predictor than age and FSH?", "CONCLUSION": "The purpose of this study is to determine if baseline antral follicle assessment may serve as additional information in predicting in vitro fertilization outcome.\nProspective, descriptive preliminary study of in vitro fertilization outcome. From July 1998 to July 1999, 224 patients underwent antral follicle assessment (follicle 2-6 mm in diameter) on baseline of the planned, stimulated in vitro fertilization cycle. The outcomes were analyzed with respect to antral follicle assessment (<or = 6 or>6), basal cycle day 3 follicle stimulated hormone (<or = 10 or>10 IU/L) and maternal age (<or = 35 or>35 years).\nThe clinical pregnancy rate was significantly higher in the group with baseline antral follicle>6 compared to that in the group with antral follicle<or = 6 (51% vs. 19%, respectively). Controlling for patient age, and basal follicle stimulated hormone, the pregnancy rate was significantly higher in the group with antral follicle>6 compared to that in the group with antral follicle<or = 6. The cancellation rate was significantly increased with advancing maternal age, elevated basal follicle stimulated hormone levels, and baseline antral follicle<or = 6. The cancellation rate was significantly higher in the group with antral follicle<or = 6 compared to that in the group with antral follicle>or = 6 (33% vs. 1%, respectively).\nIn vitro fertilization outcome is strongly correlated with both maternal ages, basal cycle, day 3 follicle, stimulated hormone, and antral follicle assessment. Antral follicle assessment was a better predictor of in vitro fertilization outcome than were age or follicle stimulated hormone. Antral follicle assessment may provide a marker for ovarian age that is distinct from chronological age or hormonal markers.\n", "TYPE": ["Adult", "Age Factors", "Female", "Fertilization in Vitro", "Follicle Stimulating Hormone", "Humans", "Logistic Models", "Male", "Ovarian Follicle", "Ovulation Induction", "Pilot Projects", "Predictive Value of Tests", "Pregnancy", "Pregnancy Outcome", "Prospective Studies", "Ultrasonography"], "FINAL_DECISION": "maybe"}
{"idx": "11867487", "QUESTION": "Does rugby headgear prevent concussion?", "CONCLUSION": "To examine the attitudes of players and coaches to the use of protective headgear, particularly with respect to the prevention of concussion.\nA questionnaire designed to assess attitudes to headgear was administered to 63 players from four different Canadian teams, each representing a different level of play (high school, university, community club, national). In addition, coaches from all four levels were questioned about team policies and their personal opinions about the use of headgear to prevent concussion.\nAlthough the players tended to believe that the headgear could prevent concussion (62%), the coaches were less convinced (33%). Despite the players' belief that headgear offers protection against concussion, only a minority reported wearing headgear (27%) and few (24%) felt that its use should be made mandatory. Common reasons for not wearing headgear were \"its use is not mandatory\", \"it is uncomfortable\", and \"it costs too much\".\nAlthough most players in the study believe that rugby headgear may prevent concussion, only a minority reported wearing it. Coaches tended to be less convinced than the players that rugby headgear can prevent concussion.\n", "TYPE": ["Adolescent", "Adult", "Athletic Injuries", "Brain Concussion", "Canada", "Female", "Football", "Head Protective Devices", "Health Knowledge, Attitudes, Practice", "Humans", "Male", "Sports Equipment", "Surveys and Questionnaires"], "FINAL_DECISION": "maybe"}
{"idx": "20971618", "QUESTION": "Are lifetime prevalence of impetigo, molluscum and herpes infection really increased in children having atopic dermatitis?", "CONCLUSION": "Cutaneous infections such as impetigo contagiosum (IC), molluscum contagiosum (MC) and herpes virus infection (HI) appear to be associated with atopic dermatitis (AD), but there are no reports of concrete epidemiological evidence.\nWe evaluated the association of childhood AD with these infections by conducting a population-based cross-sectional study.\nEnrolled in this study were 1117 children aged 0-6 years old attending nursery schools in Ishigaki City, Okinawa Prefecture, Japan. Physical examination was performed by dermatologists, and a questionnaire was completed on each child's history of allergic diseases including AD, asthma, allergic rhinitis and egg allergy, and that of skin infections including IC, MC and HI, as well as familial history of AD.\nIn 913 children (AD; 132), a history of IC, MC or HI was observed in 45.1%, 19.7%, and 2.5%, respectively. Multiple logistic regression analysis revealed that the odds of having a history of IC were 1.8 times higher in AD children than in non-AD children. Meanwhile, a history of MC was significantly correlated to the male gender, but not to a personal history of AD. As for HI, we found no correlated factors in this study.\nThe lifetime prevalence of IC was indeed higher in young children with a history of AD.\n", "TYPE": ["Child", "Child, Preschool", "Cross-Sectional Studies", "Dermatitis, Atopic", "Female", "Herpesviridae Infections", "Humans", "Impetigo", "Infant", "Japan", "Male", "Molluscum Contagiosum", "Prevalence", "Risk Factors"], "FINAL_DECISION": "maybe"}
{"idx": "16968876", "QUESTION": "Is a patient's self-reported health-related quality of life a prognostic factor for survival in non-small-cell lung cancer patients?", "CONCLUSION": "The aim of this prognostic factor analysis was to investigate if a patient's self-reported health-related quality of life (HRQOL) provided independent prognostic information for survival in non-small cell lung cancer (NSCLC) patients.\nPretreatment HRQOL was measured in 391 advanced NSCLC patients using the EORTC QLQ-C30 and the EORTC Lung Cancer module (QLQ-LC13). The Cox proportional hazards regression model was used for both univariate and multivariate analyses of survival. In addition, a bootstrap validation technique was used to assess the stability of the outcomes.\nThe final multivariate Cox regression model retained four parameters as independent prognostic factors for survival: male gender with a hazard ratio (HR) = 1.32 (95% CI 1.03-1.69; P = 0.03); performance status (0 to 1 versus 2) with HR = 1.63 (95% CI 1.04-2.54; P = 0.032); patient's self-reported score of pain with HR= 1.11 (95% CI 1.07-1.16; P<0.001) and dysphagia with HR = 1.12 (95% CI 1.04-1.21; P = 0.003). A 10-point shift worse in the scale measuring pain and dysphagia translated into an 11% and 12% increased in the likelihood of death respectively. A risk group categorization was also developed.\nThe results suggest that patients' self-reported HRQOL provide independent prognostic information for survival. This finding supports the collection of such data in routine clinical practice.\n", "TYPE": ["Adult", "Aged", "Carcinoma, Non-Small-Cell Lung", "Europe", "Female", "Health Status", "Humans", "Male", "Middle Aged", "Multivariate Analysis", "Prognosis", "Quality of Life", "Regression Analysis", "Survival Analysis"], "FINAL_DECISION": "maybe"}
{"idx": "18568290", "QUESTION": "Is there a role for endothelin-1 in the hemodynamic changes during hemodialysis?", "CONCLUSION": "The etiology of hemodialysis (HD)-induced hypotension and hypertension remains speculative. There is mounting evidence that endothelin-1 (ET-1) may play a vital role in these hemodynamic changes. We examined the possible role of intradialytic changes of ET-1 in the pathogenesis of hypotension and rebound hypertension during HD.\nThe present study included 45 patients with end-stage renal disease (ESRD) on regular HD. They were divided according to their hemodynamic status during HD into three groups (group I had stable intradialytic hemodynamics, group II had dialysis-induced hypotension, and group III had rebound hypertension during HD). In addition, 15 healthy volunteers were included as a control group. Pulse and blood pressure were monitored before, during (every half hour), and after HD session. ET-1 level was measured at the beginning, middle, and end of HD. ET-1 was measured in the control group for comparison.\nPre-dialysis levels of ET-1 were significantly higher in dialysis patients compared to the controls (P<0.001); however, they were comparable in the three HD groups. The post-dialysis ET-1 level was not changed significantly in group I compared with predialysis values (14.49 +/- 2.04 vs. 14.33 +/- 2.23 pg/ml; P = NS), while the ET-1 concentration decreased significantly in group II and increased in group III in comparison to predialysis values (8.56 +/- 1.44 vs. 11.75 +/- 2.51; 16.39 +/- 3.12 vs. 11.93 +/- 2.11 pg/ml, respectively; P<0.001).\nAltered ET-1 levels may be involved in the pathogenesis of rebound hypertension and hypotension during HD.\n", "TYPE": ["Adult", "Blood Pressure", "Case-Control Studies", "Egypt", "Endothelin-1", "Female", "Heart Rate", "Humans", "Hypertension", "Hypotension", "Kidney Failure, Chronic", "Male", "Middle Aged", "Prospective Studies", "Renal Dialysis"], "FINAL_DECISION": "maybe"}
{"idx": "16816043", "QUESTION": "Do French lay people and health professionals find it acceptable to breach confidentiality to protect a patient's wife from a sexually transmitted disease?", "CONCLUSION": "To determine under what conditions lay people and health professionals find it acceptable for a physician to breach confidentiality to protect the wife of a patient with a sexually transmitted disease (STD).\nIn a study in France, breaching confidentiality in 48 scenarios were accepted by 144 lay people, 10 psychologists and 7 physicians. The scenarios were all possible combinations of five factors: severity of the disease (severe, lethal); time taken to discuss this with (little time, much time); intent to inform the spouse about the disease (none, one of these days, immediately); intent to adopt protective behaviours (no intent, intent); and decision to consult an expert in STDs (yes, no), 2 x 2 x 3 x 2 x 2. The importance and interactions of each factor were determined, at the group level, by performing analyses of variance and constructing graphs.\nThe concept of breaching confidentiality to protect a wife from her husband's STD was favoured much more by lay people and psychologists than by physicians (mean ratings 11.76, 9.28 and 2.90, respectively, on a scale of 0-22). The patient's stated intentions to protect his wife and to inform her of the disease had the greatest impact on acceptability. A cluster analysis showed groups of lay participants who found breaching confidentiality \"always acceptable\" (n = 14), \"depending on the many circumstances\" (n = 87), requiring \"consultation with an expert\" (n = 30) and \"never acceptable (n = 13)\".\nMost people in France are influenced by situational factors when deciding if a physician should breach confidentiality to protect the spouse of a patient infected with STD.\n", "TYPE": ["Adolescent", "Adult", "Attitude of Health Personnel", "Attitude to Health", "Cluster Analysis", "Confidentiality", "Female", "France", "Humans", "Intention", "Interpersonal Relations", "Male", "Middle Aged", "Severity of Illness Index", "Sexual Behavior", "Sexually Transmitted Diseases", "Spouses", "Time Factors"], "FINAL_DECISION": "maybe"}
{"idx": "11458136", "QUESTION": "Does managed care enable more low income persons to identify a usual source of care?", "CONCLUSION": "By requiring or encouraging enrollees to obtain a usual source of care, managed care programs hope to improve access to care without incurring higher costs.\n(1) To examine the effects of managed care on the likelihood of low-income persons having a usual source of care and a usual physician, and; (2) To examine the association between usual source of care and access.\nCross-sectional survey of households conducted during 1996 and 1997.\nA nationally representative sample of 14,271 low-income persons.\nUsual source of care, usual physician, managed care enrollment, managed care penetration.\nHigh managed care penetration in the community is associated with a lower likelihood of having a usual source of care for uninsured persons (54.8% vs. 62.2% in low penetration areas) as well as a lower likelihood of having a usual physician (60% vs. 72.8%). Managed care has only marginal effects on the likelihood of having a usual source of care for privately insured and Medicaid beneficiaries. Having a usual physician substantially reduces unmet medical needs for the insured but less so for the uninsured.\nHaving a usual physician can be an effective tool in improving access to care for low-income populations, although it is most effective when combined with insurance coverage. However, the effectiveness of managed care in linking more low-income persons to a medical home is uncertain, and may have unintended consequences for uninsured persons.\n", "TYPE": ["Adult", "Continuity of Patient Care", "Cross-Sectional Studies", "Female", "Health Services Accessibility", "Humans", "Insurance Coverage", "Male", "Managed Care Programs", "Medically Uninsured", "Multivariate Analysis", "Poverty", "United States"], "FINAL_DECISION": "maybe"}
{"idx": "27044366", "QUESTION": "Detailed analysis of sputum and systemic inflammation in asthma phenotypes: are paucigranulocytic asthmatics really non-inflammatory?", "CONCLUSION": "The technique of induced sputum has allowed to subdivide asthma patients into inflammatory phenotypes according to their level of granulocyte airway infiltration. There are very few studies which looked at detailed sputum and blood cell counts in a large cohort of asthmatics divided into inflammatory phenotypes. The purpose of this study was to analyze sputum cell counts, blood leukocytes and systemic inflammatory markers in these phenotypes, and investigate how those groups compared with healthy subjects.\nWe conducted a retrospective cross-sectional study on 833 asthmatics recruited from the University Asthma Clinic of Liege and compared them with 194 healthy subjects. Asthmatics were classified into inflammatory phenotypes.\nThe total non-squamous cell count per gram of sputum was greater in mixed granulocytic and neutrophilic phenotypes as compared to eosinophilic, paucigranulocytic asthma and healthy subjects (p\u2009<\u20090.005). Sputum eosinophils (in absolute values and percentages) were increased in all asthma phenotypes including paucigranulocytic asthma, compared to healthy subjects (p\u2009<\u20090.005). Eosinophilic asthma showed higher absolute sputum neutrophil and lymphocyte counts than healthy subjects (p\u2009<\u20090.005), while neutrophilic asthmatics had a particularly low number of sputum macrophages and epithelial cells. All asthma phenotypes showed an increased blood leukocyte count compared to healthy subjects (p\u2009<\u20090.005), with paucigranulocytic asthmatics having also increased absolute blood eosinophils compared to healthy subjects (p\u2009<\u20090.005). Neutrophilic asthma had raised CRP and fibrinogen while eosinophilic asthma only showed raised fibrinogen compared to healthy subjects (p\u2009<\u20090.005).\nThis study demonstrates that a significant eosinophilic inflammation is present across all categories of asthma, and that paucigranulocytic asthma may be seen as a low grade inflammatory disease.\n", "TYPE": ["Adult", "Aged", "Asthma", "C-Reactive Protein", "Case-Control Studies", "Cross-Sectional Studies", "Eosinophils", "Female", "Fibrinogen", "Granulocytes", "Humans", "Inflammation", "Leukocyte Count", "Lymphocyte Count", "Macrophages", "Male", "Middle Aged", "Neutrophils", "Phenotype", "Retrospective Studies", "Sputum"], "FINAL_DECISION": "maybe"}
{"idx": "25779009", "QUESTION": "Bactericidal activity of 3 cutaneous/mucosal antiseptic solutions in the presence of interfering substances: Improvement of the NF EN 13727 European Standard?", "CONCLUSION": "There is no standard protocol for the evaluation of antiseptics used for skin and mucous membranes in the presence of interfering substances. Our objective was to suggest trial conditions adapted from the NF EN 13727 standard, for the evaluation of antiseptics used in gynecology and dermatology.\nThree antiseptic solutions were tested in vitro: a chlorhexidine-benzalkonium (CB) combination, a hexamidine-chlorhexidine-chlorocresol (HCC) combination, and povidone iodine (P). The adaptation of trial conditions to the standard involved choosing dilutions, solvent, and interfering substances. The activity of solutions was assessed on the recommended strains at concentrations of 97% (pure solution), 50%, and 10% (diluted solution), and 1%. A logarithmic reduction \u2265 5 was expected after 60seconds of contact, to meet requirements of bactericidal activity.\nHCC did not present any bactericidal activity except on P. aeruginosa at a concentration of 97%. P was not bactericidal on E. hirae at any concentration and on S. aureus at 97%. CB had the most homogeneous bactericidal activity with a reduction>5 log on the 4 bacterial strains at concentrations of 97%, 50% and 10%.\nAdapting the NF EN 13727 standard allowed assessing the 3 tested solutions: only CB was bactericidal in dirty conditions. This study proved the possibility of validating antiseptic choice in vitro, in current practice conditions, for adjunctive treatment of skin and mucous membranes disorders, primarily of bacterial origin or with a potential of superinfection.\n", "TYPE": ["Animals", "Anti-Infective Agents, Local", "Benzalkonium Compounds", "Benzamidines", "Cattle", "Chlorhexidine", "Cresols", "Dose-Response Relationship, Drug", "Drug Combinations", "Drug Interactions", "Enterococcus", "Erythrocytes", "Escherichia coli", "Europe", "Hand Disinfection", "Humans", "Inorganic Chemicals", "Microbial Sensitivity Tests", "Mucous Membrane", "Osmolar Concentration", "Povidone-Iodine", "Pseudomonas aeruginosa", "Serum Albumin, Bovine", "Skin", "Solutions", "Staphylococcus aureus"], "FINAL_DECISION": "maybe"}
